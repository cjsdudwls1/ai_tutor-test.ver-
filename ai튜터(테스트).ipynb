{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1QaCSoYhP2FfXGE_VkcImuQTyKFC1G-Hk",
      "authorship_tag": "ABX9TyMOaZ/s983Zg3px5X4pQDS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjsdudwls1/ai_tutor-test.ver-/blob/main/ai%ED%8A%9C%ED%84%B0(%ED%85%8C%EC%8A%A4%ED%8A%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFpz_H5dEqQX",
        "outputId": "f27c1e87-876b-4643-c08e-93f5a6798a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지 설치 (버전 명시)\n",
        "!pip install faiss-cpu langchain-text-splitters python-docx unstructured langchain_community llama-index gradio docx2txt -qqq\n",
        "!pip install --upgrade \"google-genai>=1.7.0\" \"langchain-google-genai>=2.1.3\" -qqq\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tDJ_m7Aq6XN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1단계 : 문서 로드**"
      ],
      "metadata": {
        "id": "mc4Z5XeFj3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "\n",
        "loader = Docx2txtLoader(\"/content/drive/MyDrive/(난이도표시본) 샘플문제 워드로 다시.docx\")  # 문서 로더 초기화\n",
        "\n",
        "docs = loader.load()  # 문서 로딩\n",
        "\n",
        "print(len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "demLNfoK4SEv",
        "outputId": "662a417e-c693-4500-e5c8-0e2004d1188f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llm이 못알아 들을 수 있으니 <br>**정규 표현식을 사용하여 '하 N.', '중 N.', '상 N.'을 '난이도 하 N.', '난이도 중 N.', '난이도 상 N.'으로 변환**"
      ],
      "metadata": {
        "id": "EtcOc1UR0gI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def transform_difficulty_markers(docs):\n",
        "    for i, doc in enumerate(docs):\n",
        "        modified_content = re.sub(r'(하|중|상)( \\d+\\.)', r'난이도 \\1\\2', doc.page_content)\n",
        "        docs[i].page_content = modified_content\n",
        "    return docs\n",
        "\n",
        "# 변환된 문서 사용\n",
        "docs = transform_difficulty_markers(docs)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv2ry88STBCR",
        "outputId": "81a350f6-3bfa-47fe-c104-3e4c08f60f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플문제 (정답 및 해설 포함)\n",
            "\n",
            "난이도 하 1. 다음 중 간(liver)에서 수행하지 않는 주요 기능은 무엇인가?\n",
            "\n",
            "A. 알부민(albumin) 합성\n",
            "\n",
            "B. 담즙(bile) 생성\n",
            "\n",
            "C. 혈당 조절(glucose regulation)\n",
            "\n",
            "D. 인슐린(insulin) 분비\n",
            "\n",
            "정답: D. 인슐린(insulin) 분비\n",
            " 해설: 인슐린은 췌장(이자)에서 분비되며, 간은 알부민 합성, 담즙 생성, 혈당 조절을 담당합니다.\n",
            "\n",
            "난이도 중 2. 근수축(muscle contraction)에 직접적으로 관여하는 단백질 복합체로 옳은 것은?\n",
            "\n",
            "A. 미오신(myosin)과 트로포닌(troponin)\n",
            "\n",
            "B. 트로포닌(troponin)과 트로포마이오신(tropomyosin)\n",
            "\n",
            "C. 액틴(actin)과 콜라겐(collagen)\n",
            "\n",
            "D. 엘라스틴(elastin)과 미오신(myosin)\n",
            "\n",
            "정답: B. 트로포닌(troponin)과 트로포마이오신(tropomyosin)\n",
            " 해설: 트로포닌과 트로포마이오신은 근수축 시 칼슘과 결합하여 액틴-미오신 상호작용을 조절합니다.\n",
            "\n",
            "난이도 하 3. 다음 중 혈액-뇌 장벽(blood-brain barrier)을 통과하기 가장 쉬운 물질은?\n",
            "\n",
            "A. 포도당(glucose)\n",
            "\n",
            "B. 산소(oxygen)\n",
            "\n",
            "C. 단백질(protein)\n",
            "\n",
            "D. 나트륨 이온(sodium ion)\n",
            "\n",
            "정답: B. 산소(oxygen)\n",
            " 해설: 산소는 지용성 분자로, 혈액-뇌 장벽을 가장 쉽게 통과할 수 있습니다.\n",
            "\n",
            "난이도 중 4. 신장에서 여과(filtration)가 일어나는 주요 구조물은?\n",
            "\n",
            "A. 보우만주머니(Bowman's capsule)\n",
            "\n",
            "B. 근위세뇨관(proximal convoluted tubule)\n",
            "\n",
            "C. 헨레고리(loop of Henle)\n",
            "\n",
            "D. 집합관(collecting duct)\n",
            "\n",
            "정답: A. 보우만주머니(Bowman's capsule)\n",
            " 해설: 신장에서 혈액의 여과는 사구체와 보우만주머니에서 일어납니다.\n",
            "\n",
            "난이도 중 5. 다음 중 자율신경계(autonomic nervous system)의 교감신경계(sympathetic nervous system)의 반응이 아닌 것은?\n",
            "\n",
            "A. 동공 확대(pupil dilation)\n",
            "\n",
            "B. 심박수 증가(increased heart rate)\n",
            "\n",
            "C. 소화활동 촉진(enhanced digestion)\n",
            "\n",
            "D. 기관지 확장(bronchodilation)\n",
            "\n",
            "정답: C. 소화활동 촉진(enhanced digestion)\n",
            " 해설: 교감신경계는 소화 활동을 억제하고, 부교감신경계가 소화활동을 촉진합니다.\n",
            "\n",
            "난이도 상 6. 호르몬 조절(hormonal regulation)에 있어 음성되먹임(negative feedback)의 예로 적절한 것은?\n",
            "\n",
            "A. 혈당 증가 시 글루카곤(glucagon) 분비 증가\n",
            "\n",
            "B. 혈중 칼슘 농도 감소 시 칼시토닌(calcitonin) 분비 증가\n",
            "\n",
            "C. 갑상선 호르몬(thyroid hormone) 증가 시 TSH 감소\n",
            "\n",
            "D. 체온 증가 시 땀샘 분비 감소\n",
            "\n",
            "정답: C. 갑상선 호르몬(thyroid hormone) 증가 시 TSH 감소\n",
            " 해설: 갑상선 호르몬이 증가하면 뇌하수체에서 TSH 분비가 억제되는 것이 대표적인 음성 피드백입니다.\n",
            "\n",
            "난이도 중 7. 근육의 유형 중 심장근(cardiac muscle)에 대한 설명으로 옳지 않은 것은?\n",
            "\n",
            "A. 불수의근(involuntary muscle)이다.\n",
            "\n",
            "B. 간극결합(gap junction)을 포함한다.\n",
            "\n",
            "C. 수축은 자율신경계에 의해 조절된다.\n",
            "\n",
            "D. 핵이 다수 존재한다.\n",
            "\n",
            "정답: D. 핵이 다수 존재한다.\n",
            " 해설: 심장근은 대부분 단일 핵을 가지며 다수의 핵은 골격근의 특징입니다.\n",
            "\n",
            "난이도 하 8. 혈액의 항상성(homeostasis)을 유지하는 과정에서 혈소판(platelets)의 주요 역할은?\n",
            "\n",
            "A. 산소 운반\n",
            "\n",
            "B. 면역 반응 촉진\n",
            "\n",
            "C. 혈액 응고 시작\n",
            "\n",
            "D. 삼투압 유지\n",
            "\n",
            "정답: C. 혈액 응고 시작\n",
            " 해설: 혈소판은 혈관이 손상되었을 때 혈액응고를 시작하는 역할을 합니다.\n",
            "\n",
            "난이도 중 9. 신경세포(neuron)의 활동전위(action potential)에서 탈분극(depolarization)이 일어나는 원인은?\n",
            "\n",
            "A. 칼슘 이온의 유입\n",
            "\n",
            "B. 칼륨 이온의 유출\n",
            "\n",
            "C. 나트륨 이온의 유입\n",
            "\n",
            "D. 염소 이온의 유입\n",
            "\n",
            "정답: C. 나트륨 이온의 유입\n",
            " 해설: 나트륨 이온이 세포 안으로 들어오면서 막 전위가 양전하로 변하는 것이 탈분극입니다.\n",
            "\n",
            "난이도 중 10. 다음 중 간뇌(diencephalon)에 속하는 구조물이 아닌 것은?\n",
            "\n",
            "A. 시상(thalamus)\n",
            "\n",
            "B. 시상하부(hypothalamus)\n",
            "\n",
            "C. 송과선(pineal gland)\n",
            "\n",
            "D. 연수(medulla oblongata)\n",
            "\n",
            "정답: D. 연수(medulla oblongata)\n",
            " 해설: 연수는 뇌간에 속하며, 간뇌에는 시상, 시상하부, 송과선이 포함됩니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2단계: 텍스트 분할**"
      ],
      "metadata": {
        "id": "8yUwnpBuQqRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "\n",
        "# 2. 청킹을 위한 정규식 패턴 정의\n",
        "pattern = r'(난이도 하 \\d+\\.|난이도 중 \\d+\\.|난이도 상 \\d+\\.)'\n",
        "\n",
        "# 3. 커스텀 분할 함수\n",
        "def question_splitter(text, pattern):\n",
        "    splits = re.split(pattern, text)\n",
        "    chunks = []\n",
        "    for i in range(1, len(splits), 2):\n",
        "        if i+1 < len(splits):\n",
        "            chunk = splits[i] + splits[i+1]\n",
        "            chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# 4. 문서 내용 추출 및 분할\n",
        "full_text = docs[0].page_content  # 로드된 문서의 전체 텍스트\n",
        "question_chunks = question_splitter(full_text, pattern)\n",
        "\n",
        "# 5. 결과 확인\n",
        "for idx, chunk in enumerate(question_chunks[:3]):  # 상위 3개 샘플 출력\n",
        "    print(f\"=== 문제 {idx+1} ===\")\n",
        "    print(chunk)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c05YqO-4SMB",
        "outputId": "34608a77-461f-4990-9e93-2f21d3a85be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 문제 1 ===\n",
            "난이도 하 1. 다음 중 간(liver)에서 수행하지 않는 주요 기능은 무엇인가?\n",
            "\n",
            "A. 알부민(albumin) 합성\n",
            "\n",
            "B. 담즙(bile) 생성\n",
            "\n",
            "C. 혈당 조절(glucose regulation)\n",
            "\n",
            "D. 인슐린(insulin) 분비\n",
            "\n",
            "정답: D. 인슐린(insulin) 분비\n",
            " 해설: 인슐린은 췌장(이자)에서 분비되며, 간은 알부민 합성, 담즙 생성, 혈당 조절을 담당합니다.\n",
            "\n",
            "\n",
            "=== 문제 2 ===\n",
            "난이도 중 2. 근수축(muscle contraction)에 직접적으로 관여하는 단백질 복합체로 옳은 것은?\n",
            "\n",
            "A. 미오신(myosin)과 트로포닌(troponin)\n",
            "\n",
            "B. 트로포닌(troponin)과 트로포마이오신(tropomyosin)\n",
            "\n",
            "C. 액틴(actin)과 콜라겐(collagen)\n",
            "\n",
            "D. 엘라스틴(elastin)과 미오신(myosin)\n",
            "\n",
            "정답: B. 트로포닌(troponin)과 트로포마이오신(tropomyosin)\n",
            " 해설: 트로포닌과 트로포마이오신은 근수축 시 칼슘과 결합하여 액틴-미오신 상호작용을 조절합니다.\n",
            "\n",
            "\n",
            "=== 문제 3 ===\n",
            "난이도 하 3. 다음 중 혈액-뇌 장벽(blood-brain barrier)을 통과하기 가장 쉬운 물질은?\n",
            "\n",
            "A. 포도당(glucose)\n",
            "\n",
            "B. 산소(oxygen)\n",
            "\n",
            "C. 단백질(protein)\n",
            "\n",
            "D. 나트륨 이온(sodium ion)\n",
            "\n",
            "정답: B. 산소(oxygen)\n",
            " 해설: 산소는 지용성 분자로, 혈액-뇌 장벽을 가장 쉽게 통과할 수 있습니다.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3단계: 임베딩**"
      ],
      "metadata": {
        "id": "jvkDZ65_7mm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 1. 모델과 토크나이저 로드\n",
        "model_name = 'BM-K/KoSimCSE-roberta'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. 임베딩 추출 함수 수정 (문장 수준 임베딩)\n",
        "def get_embeddings(sentences):\n",
        "    # 입력: 문장 리스트 (예: [\"문장1\", \"문장2\", ...])\n",
        "    inputs = tokenizer(\n",
        "        sentences,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,  # 최대 길이 제한\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [CLS] 토큰 임베딩 추출 (첫 번째 토큰)\n",
        "    # outputs.last_hidden_state.shape = (batch_size, seq_len, hidden_dim)\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # 정규화 (선택 사항)\n",
        "    cls_embeddings = torch.nn.functional.normalize(cls_embeddings, p=2, dim=1)\n",
        "\n",
        "    return cls_embeddings\n",
        "\n",
        "# 3. 예시: 청킹된 문제 리스트를 임베딩\n",
        "embeddings = get_embeddings(question_chunks)  # (num_chunks, 768) 형태 텐서\n",
        "\n",
        "# 4. 임베딩 결과 확인\n",
        "print(embeddings.shape)  # → torch.Size([문항개수, 768])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJWp-tl4SQw",
        "outputId": "cc8af081-f3fc-4ed4-d60e-f17c661c1242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4단계: FAISS 벡터 스토어 생성**"
      ],
      "metadata": {
        "id": "evf_f6Lh_HY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 저장 경로 설정\n",
        "save_dir = '/content/drive/MyDrive/tutor_store'\n",
        "\n",
        "# 1. 임베딩을 NumPy 배열로 변환 (KoSimCSE 출력이 PyTorch 텐서라 가정)\n",
        "embeddings_np = embeddings.numpy().astype('float32')  # (num_chunks, 768)\n",
        "\n",
        "# 2. FAISS 인덱스 생성 (L2 거리 기준)\n",
        "dimension = embeddings_np.shape[1]  # 768\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# 3. 임베딩을 인덱스에 추가\n",
        "faiss_index.add(embeddings_np)\n",
        "\n",
        "# 4. 벡터스토어와 청크 매핑 저장\n",
        "faiss_path = os.path.join(save_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(save_dir, \"chunk_mapping.pkl\")\n",
        "\n",
        "# FAISS 인덱스 저장\n",
        "faiss.write_index(faiss_index, faiss_path)\n",
        "print(f\"FAISS 인덱스가 {faiss_path}에 저장되었습니다.\")\n",
        "\n",
        "# 청크 리스트 저장 (검색 시 텍스트 복원용)\n",
        "with open(chunk_path, 'wb') as f:\n",
        "    pickle.dump(question_chunks, f)\n",
        "print(f\"청크 매핑이 {chunk_path}에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOn1UZpw4SXX",
        "outputId": "888262c3-755d-4541-cbfe-90ddea502fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS 인덱스가 /content/drive/MyDrive/tutor_store/medical_qa_index.faiss에 저장되었습니다.\n",
            "청크 매핑이 /content/drive/MyDrive/tutor_store/chunk_mapping.pkl에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5단계: tutor 시스템 구현 (gemini-1.5-pro)"
      ],
      "metadata": {
        "id": "EAuwqYyvWbO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **라이브러리 임포트 및 기본 설정**"
      ],
      "metadata": {
        "id": "kaTOxXlE7IwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import gradio as gr\n",
        "from google.colab import drive, userdata\n",
        "from typing import List, Dict, Optional, Any\n",
        "from langchain_community.vectorstores.faiss import FAISS as LangchainFAISS\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import FakeEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "\n",
        "# 드라이브 마운트 및 API 키 설정\n",
        "drive.mount('/content/drive')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('Gemini_API_KEY')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lRVxIZC0FE9",
        "outputId": "f42bd2f7-2771-462d-bc8e-3453acbae1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터 스토어 경로 설정**"
      ],
      "metadata": {
        "id": "DSyaObKd7N8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_dir = '/content/drive/MyDrive/tutor_store'\n",
        "faiss_path = os.path.join(store_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(store_dir, \"chunk_mapping.pkl\")"
      ],
      "metadata": {
        "id": "9Q2kMYpY7JrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **세션 상태 관리 클래스**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. SessionState 클래스는 사용자의 난이도(current_level), 정답/오답 개수, 현재 문제·정답 보관, 이미 본 문제 ID 집합을 관리합니다.\n",
        "\n",
        "2. total_attempts, success_rate 프로퍼티로 시도 횟수와 성공률을 계산합니다.\n",
        "\n",
        "3. update_stats() 메서드로 정답 여부에 따라 카운트를 갱신하고, 일정 기준 이상일 때 난이도를 상향/하향 조정합니다."
      ],
      "metadata": {
        "id": "jJJTVsD37dmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.current_level = \"난이도 하\"\n",
        "        self.correct_count = 0\n",
        "        self.incorrect_count = 0\n",
        "        self.current_question = None\n",
        "        self.current_answer = None\n",
        "        self.question_ids = set()\n",
        "\n",
        "    @property\n",
        "    def total_attempts(self):\n",
        "        return self.correct_count + self.incorrect_count\n",
        "\n",
        "    @property\n",
        "    def success_rate(self):\n",
        "        return (self.correct_count / self.total_attempts) if self.total_attempts else 0\n",
        "\n",
        "    def update_stats(self, is_correct: bool):\n",
        "        if is_correct:\n",
        "            self.correct_count += 1\n",
        "            if self.current_level == \"난이도 하\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"난이도 중\"\n",
        "            elif self.current_level == \"난이도 중\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"난이도 상\"\n",
        "        else:\n",
        "            self.incorrect_count += 1\n",
        "            if self.current_level == \"난이도 상\" and self.success_rate < 0.5:\n",
        "                self.current_level = \"난이도 중\"\n",
        "            elif self.current_level == \"난이도 중\" and self.success_rate < 0.3:\n",
        "                self.current_level = \"난이도 하\"\n"
      ],
      "metadata": {
        "id": "WrSh8LCd7XUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **MedicalQuestionSplitter (커스텀 텍스트 분할기)**\n",
        "1. TextSplitter를 상속하여, “난이도 하 1.”처럼 각 문제 제목 패턴별로 텍스트를 분할합니다.\n",
        "\n",
        "2. split_text()는 정규표현식으로 짝수 인덱스의 제목과 문제 본문을 합쳐서 청크(문제 단위) 리스트를 반환합니다."
      ],
      "metadata": {
        "id": "D-7VQA2u7kj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalQuestionSplitter(TextSplitter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pattern = r'(난이도 하 \\d+\\.|난이도 중 \\d+\\.|난이도 상 \\d+\\.)'\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        splits = re.split(self.pattern, text)\n",
        "        chunks = []\n",
        "        for i in range(1, len(splits), 2):\n",
        "            if i+1 < len(splits):\n",
        "                chunk = splits[i] + splits[i+1]\n",
        "                chunks.append(chunk.strip())\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "H36KIbOJ7kn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CustomFakeEmbeddings (커스텀 임베딩)**\n",
        "\n",
        "1.\n",
        "FAISS 인덱스 로드 시, 실제 임베딩 대신 모두 0벡터를 반환하는 FakeEmbeddings를 활용합니다.\n",
        "\n",
        "2. embed_documents(), embed_query() 모두 고정 길이 0벡터를 생성하여 반환합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "0pIm8x6I7uaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomFakeEmbeddings(FakeEmbeddings):\n",
        "    def __init__(self, size: int = 1024):\n",
        "        super().__init__(size=size)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [np.zeros(self.size) for _ in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return np.zeros(self.size)"
      ],
      "metadata": {
        "id": "bKTIVq3U7phB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터스토어 및 문서 로드 함수**\n",
        "1. FAISS 인덱스 파일을 읽어오고, pickle로 저장된 문제 청크 리스트를 불러옵니다.\n",
        "\n",
        "2. 각 청크를 Document 객체로 래핑하며, 레벨 정보는 get_level_from_text()로 메타데이터에 저장합니다.\n",
        "\n",
        "3. 앞서 정의한 CustomFakeEmbeddings를 사용해 LangChain FAISS 벡터스토어를 초기화한 후, docstore에 문서를 매핑합니다."
      ],
      "metadata": {
        "id": "Ykyj6Qof7yvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vectorstore():\n",
        "    print(\"FAISS 인덱스와 청크 매핑을 로드하는 중...\")\n",
        "    index = faiss.read_index(faiss_path)\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        question_chunks = pickle.load(f)\n",
        "    documents = [Document(page_content=chunk, metadata={\"level\": get_level_from_text(chunk)}) for chunk in question_chunks]\n",
        "    embeddings = CustomFakeEmbeddings(size=index.d)\n",
        "    vectorstore = LangchainFAISS(\n",
        "        embedding_function=embeddings,\n",
        "        index=index,\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={}\n",
        "    )\n",
        "    vectorstore.docstore._dict = {str(i): doc for i, doc in enumerate(documents)}\n",
        "    return vectorstore, documents\n"
      ],
      "metadata": {
        "id": "W1nuAPsb7zAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **유틸리티 함수들**\n",
        "1. get_level_from_text(): 텍스트 맨 앞의 “난이도” 정보를 추출합니다.\n",
        "\n",
        "2. process_question(): 본문에서 문제, 정답, 해설을 분리하여 튜플로 반환합니다.\n",
        "\n",
        "3. hide_answer_from_question(): 문제 텍스트에서 정답 부분을 제거해 학생에게만 질문만 보여줍니다."
      ],
      "metadata": {
        "id": "cqFRFbcL78p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_level_from_text(text: str) -> str:\n",
        "    if text.startswith(\"난이도 하\"): return \"난이도 하\"\n",
        "    if text.startswith(\"난이도 중\"): return \"난이도 중\"\n",
        "    if text.startswith(\"난이도 상\"): return \"난이도 상\"\n",
        "    match = re.match(r'(난이도\\s*[하중상])', text)\n",
        "    return match.group(1) if match else \"난이도 하\"\n",
        "\n",
        "def process_question(text: str) -> tuple:\n",
        "    if \"정답:\" not in text:\n",
        "        return text, \"\", \"\"\n",
        "    q_part = text.split(\"정답:\")[0].strip()\n",
        "    a_full = text.split(\"정답:\")[1].split(\"\\n\")[0].strip()\n",
        "    a_part = a_full.split('.')[0].strip() if '.' in a_full else a_full\n",
        "    exp = text.split(\"해설:\")[1].strip() if \"해설:\" in text else \"\"\n",
        "    return q_part, a_part, exp\n",
        "\n",
        "def hide_answer_from_question(text: str) -> str:\n",
        "    return text.split(\"정답:\")[0].strip() if \"정답:\" in text else text\n"
      ],
      "metadata": {
        "id": "Fdifk7vl78w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAG(검색연계생성) 체인 및 리트리버 설정**"
      ],
      "metadata": {
        "id": "2C-opp897__d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_chain():\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "    vectorstore, documents = load_vectorstore()\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    return llm, retriever, documents\n"
      ],
      "metadata": {
        "id": "_rxisxAL8AEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **사용자 맞춤 질문 선택기 생성**\n",
        "1. 현재 세션의 난이도와 이미 본 문제 목록을 기준으로, 아직 보지 않은 문제를 필터링합니다.\n",
        "\n",
        "2. 해당 난이도의 문제가 부족하면 다른 난이도로 변경하거나, 모두 소진 시 question_ids를 초기화해 재출제합니다.\n",
        "\n",
        "3. 선택된 문서를 반환하는 CustomRetriever 클래스를 정의하여 LangChain 리트리버 인터페이스와 호환시킵니다."
      ],
      "metadata": {
        "id": "IBFFamLF8ITb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_question_selector(documents: List[Document], session: SessionState):\n",
        "    def filter_by_level_and_seen(query: str):\n",
        "        level = session.current_level\n",
        "        filtered_docs = [\n",
        "            doc for doc in documents\n",
        "            if doc.metadata.get(\"level\") == level and hash(doc.page_content) not in session.question_ids\n",
        "        ]\n",
        "        if not filtered_docs:\n",
        "            for lvl in [\"난이도 하\", \"난이도 중\", \"난이도 상\"]:\n",
        "                if lvl != level:\n",
        "                    filtered_docs = [\n",
        "                        doc for doc in documents\n",
        "                        if doc.metadata.get(\"level\") == lvl and hash(doc.page_content) not in session.question_ids\n",
        "                    ]\n",
        "                    if filtered_docs:\n",
        "                        session.current_level = lvl\n",
        "                        break\n",
        "            if not filtered_docs:\n",
        "                session.question_ids.clear()\n",
        "                filtered_docs = [doc for doc in documents if doc.metadata.get(\"level\") == session.current_level]\n",
        "        if filtered_docs:\n",
        "            selected = np.random.choice(filtered_docs)\n",
        "            session.question_ids.add(hash(selected.page_content))\n",
        "            return [selected]\n",
        "        return []\n",
        "    class CustomRetriever:\n",
        "        def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            return filter_by_level_and_seen(query)\n",
        "    return CustomRetriever()\n"
      ],
      "metadata": {
        "id": "Ea_vIIEZ8ItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **프롬프트 템플릿 정의**\n",
        "1. 출제용과 피드백용 두 가지 ChatPrompt 템플릿을 정의합니다.\n",
        "\n",
        "2. question_template은 문제만 학생에게 보여주고, feedback_template은 정답 판단 및 해설에 따라 LLM에게 피드백 생성 지침을 제공합니다."
      ],
      "metadata": {
        "id": "IGIMUgGS8QEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "다음은 작업치료사 국가고시 문제입니다:\n",
        "\n",
        "{question}\n",
        "\n",
        "이 문제를 학생에게 출제하되, 정답과 해설은 포함하지 마세요.\n",
        "\"\"\")\n",
        "\n",
        "feedback_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 작업치료사 국가고시 전문가입니다.\n",
        "     학생의 답변을 평가하고 유익한 피드백을 제공해주세요.\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    문제: {question}\n",
        "    학생 답변: {user_answer}\n",
        "    정답: {correct_answer}\n",
        "    정답 여부: {is_correct}\n",
        "    해설: {explanation}\n",
        "\n",
        "    다음 지침에 따라 피드백을 제공해주세요:\n",
        "    1. 학생의 이해도를 격려하세요\n",
        "    2. 왜 해당 답변이 맞거나 틀렸는지 간략히 설명하세요\n",
        "    3. 핵심 개념을 한 문장으로 요약하세요\n",
        "    \"\"\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "4ZcIv3t18QIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **피드백 체인 생성 함수**\n",
        "1. 최초 호출 시에만 LLM과 feedback_template을 연결한 체인을 생성하고 캐시합니다.\n",
        "2. 이후로는 생성된 체인을 재활용하여 효율을 높입니다."
      ],
      "metadata": {
        "id": "G29i24S18b9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feedback_chain():\n",
        "    if not hasattr(get_feedback_chain, \"chain\"):\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "        get_feedback_chain.chain = (\n",
        "            RunnablePassthrough()\n",
        "            | feedback_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    return get_feedback_chain.chain\n"
      ],
      "metadata": {
        "id": "vUFvTWO-8cCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **문제 선택 및 질문 준비 함수**\n",
        "1. select_next_question(): question_selector를 통해 다음 문제 Document를 가져옵니다.\n",
        "\n",
        "2. prepare_next_question(): 학습 진행 상황을 문자열로 조합하고, 다음 문제만 학생에게 보여줄 형태로 반환합니다."
      ],
      "metadata": {
        "id": "v-iRatdE8g1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_next_question(session: SessionState, question_selector) -> Optional[Document]:\n",
        "    docs = question_selector.get_relevant_documents(\"next_question\")\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "def prepare_next_question(session: SessionState, question_selector) -> str:\n",
        "    next_doc = select_next_question(session, question_selector)\n",
        "    if not next_doc:\n",
        "        return \"**모든 문제를 풀었습니다! 수고하셨습니다.**\"\n",
        "    q, a, exp = process_question(next_doc.page_content)\n",
        "    session.current_question = next_doc.page_content\n",
        "    session.current_answer = a\n",
        "    q_only = hide_answer_from_question(q)\n",
        "    info = (f\"**학습 진행 상황**\\n\"\n",
        "            f\"- 현재 난이도: {session.current_level}\\n\"\n",
        "            f\"- 푼 문제: {session.total_attempts}문제\\n\"\n",
        "            f\"- 맞춘 문제: {session.correct_count}문제\\n\"\n",
        "            f\"- 틀린 문제: {session.incorrect_count}문제\\n\\n\")\n",
        "    return info + f\"**다음 문제**\\n{q_only}\"\n"
      ],
      "metadata": {
        "id": "sa49Ktp98g8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **피드백 생성 함수**\n",
        "1. 학생 답안을 받아 정답 여부 플래그와 함께 LLM 피드백 체인에 전달하여 설명을 생성합니다.\n",
        "\n",
        "2. 예외 발생 시 기본 텍스트 피드백으로 대체하고, 오답일 경우 정답과 해설을 추가로 붙여줍니다."
      ],
      "metadata": {
        "id": "L34YKScB8nQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feedback(session: SessionState, user_answer: str, is_correct: bool) -> str:\n",
        "    try:\n",
        "        question, _, exp = process_question(session.current_question)\n",
        "        feedback_input = {\n",
        "            \"question\": question,\n",
        "            \"user_answer\": user_answer,\n",
        "            \"correct_answer\": session.current_answer,\n",
        "            \"is_correct\": \"정답입니다!\" if is_correct else \"오답입니다.\",\n",
        "            \"explanation\": exp if exp else \"추가 해설 없음\"\n",
        "        }\n",
        "        fb = get_feedback_chain().invoke(feedback_input)\n",
        "    except Exception as e:\n",
        "        print(f\"피드백 생성 중 오류 발생: {e}\")\n",
        "        fb = (\"정답입니다!\" if is_correct else \"오답입니다.\") + f\" 정답은 {session.current_answer}입니다.\"\n",
        "    if not is_correct:\n",
        "        fb += f\"\\n\\n**정답**: {session.current_answer}\"\n",
        "    _, _, exp = process_question(session.current_question)\n",
        "    if exp:\n",
        "        fb += f\"\\n\\n**해설**: {exp}\"\n",
        "    return fb\n"
      ],
      "metadata": {
        "id": "boXYKqJE8nXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradio 채팅 인터페이스 로직**\n",
        "사용자가 첫 메시지를 보낼 때는 인사와 첫 문제를 보여주고, 이후로는 사용자의 답안을 평가·피드백 생성·다음 문제 제시 과정을 수행합니다."
      ],
      "metadata": {
        "id": "0Izx-0hl8s3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(message, history, session_state, question_selector):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "    if not history:\n",
        "        doc = select_next_question(session_state, question_selector)\n",
        "        if not doc:\n",
        "            return [{\"role\": \"assistant\", \"content\": \"문제를 불러올 수 없습니다.\"}], \"\"\n",
        "        q, a, _ = process_question(doc.page_content)\n",
        "        session_state.current_question = doc.page_content\n",
        "        session_state.current_answer = a\n",
        "        return [\n",
        "            {\"role\": \"assistant\", \"content\": \"안녕하세요! 작업치료사 국가고시 문제 풀이를 시작합니다.\"},\n",
        "            {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "        ], \"\"\n",
        "    new_hist = history.copy()\n",
        "    user_ans = message.strip().upper()\n",
        "    new_hist.append({\"role\": \"user\", \"content\": user_ans})\n",
        "    correct = (user_ans == session_state.current_answer.upper())\n",
        "    session_state.update_stats(correct)\n",
        "    fb = generate_feedback(session_state, user_ans, correct)\n",
        "    nxt_q = prepare_next_question(session_state, question_selector)\n",
        "    new_hist.append({\"role\": \"assistant\", \"content\": fb + \"\\n\\n\" + nxt_q})\n",
        "    return new_hist, \"\"\n"
      ],
      "metadata": {
        "id": "xsrjjRPL8s9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **버튼 클릭 및 채팅 초기화 함수**\n",
        "1. answer_click()는 버튼 클릭 시 답안을 처리하고 채팅 내역을 갱신합니다.\n",
        "\n",
        "2. clear_chat()는 세션과 채팅 내역을 초기화하여 첫 문제부터 다시 시작합니다."
      ],
      "metadata": {
        "id": "USHMe0-a8w-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_click(answer, history, session_state, question_selector):\n",
        "    new_history, _ = chat_interface(answer, history, session_state, question_selector)\n",
        "    return new_history\n",
        "\n",
        "def clear_chat(session_state, question_selector):\n",
        "    new_sess = SessionState()\n",
        "    first_doc = select_next_question(new_sess, question_selector)\n",
        "    if not first_doc:\n",
        "        return new_sess, [{\"role\": \"assistant\", \"content\": \"문제를 불러올 수 없습니다.\"}]\n",
        "    q, a, _ = process_question(first_doc.page_content)\n",
        "    new_sess.current_question = first_doc.page_content\n",
        "    new_sess.current_answer = a\n",
        "    return new_sess, [\n",
        "        {\"role\": \"assistant\", \"content\": \"안녕하세요! 작업치료사 국가고시 문제 풀이를 시작합니다.\"},\n",
        "        {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "fNzb2tgS8xDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **메인 애플리케이션 및 Gradio UI 구성**\n",
        "1. RAG 체인·세션·질문 선택기를 준비한 후, Gradio Blocks로 전체 UI를 구성합니다.\n",
        "\n",
        "2. 답안 버튼(A/B/C/D)과 초기화 버튼을 배치하고, 각 클릭 이벤트에 앞서 정의한 함수를 바인딩합니다.\n",
        "\n",
        "3. 앱을 외부 공유 모드(share=True)로 실행합니다."
      ],
      "metadata": {
        "id": "yblJz5vL81V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    llm, retriever, documents = setup_rag_chain()\n",
        "    session_state = SessionState()\n",
        "    question_selector = create_question_selector(documents, session_state)\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "        .gradio-container {max-width: 900px; margin: auto;}\n",
        "        .chat-history {height: 600px !important;}\n",
        "        footer {display: none !important;}\n",
        "    \"\"\") as demo:\n",
        "        gr.Markdown(\"# 🎓 작업치료사 국가고시 문제 학습 챗봇\")\n",
        "        gr.Markdown(\"버튼을 눌러 답안을 선택하세요.\")\n",
        "        session_state_gr = gr.State(session_state)\n",
        "        question_selector_gr = gr.State(question_selector)\n",
        "        chatbot = gr.Chatbot(label=\"학습 대화\", height=500, type=\"messages\", render_markdown=True)\n",
        "        with gr.Row():\n",
        "            btn_A = gr.Button(\"A\")\n",
        "            btn_B = gr.Button(\"B\")\n",
        "            btn_C = gr.Button(\"C\")\n",
        "            btn_D = gr.Button(\"D\")\n",
        "            clear_btn = gr.Button(\"초기화\")\n",
        "        btn_A.click(\n",
        "            lambda history, ss, qs: answer_click(\"A\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_B.click(\n",
        "            lambda history, ss, qs: answer_click(\"B\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_C.click(\n",
        "            lambda history, ss, qs: answer_click(\"C\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_D.click(\n",
        "            lambda history, ss, qs: answer_click(\"D\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        clear_btn.click(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "        demo.load(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "    demo.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "PSd83oG681dX",
        "outputId": "5843d51b-f8f0-46b4-816b-04ee30703f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS 인덱스와 청크 매핑을 로드하는 중...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c55bdcff5ccafc76c9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c55bdcff5ccafc76c9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전체코드"
      ],
      "metadata": {
        "id": "_0CuHAx2_KnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "아래의 코드를 구간 별 순서대로 설명해줘. 너가 해준 설명과 구간화를 그대로 나의COLAP작업환경에 셀로 구분하여 올릴거야.\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import gradio as gr\n",
        "from google.colab import drive, userdata\n",
        "from typing import List, Dict, Optional, Any\n",
        "from langchain_community.vectorstores.faiss import FAISS as LangchainFAISS\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import FakeEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "# 드라이브 마운트 및 API 키 설정\n",
        "drive.mount('/content/drive')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('Gemini_API_KEY')\n",
        "\n",
        "# 벡터스토어 경로 설정\n",
        "store_dir = '/content/drive/MyDrive/tutor_store'\n",
        "faiss_path = os.path.join(store_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(store_dir, \"chunk_mapping.pkl\")\n",
        "\n",
        "# 세션 상태 저장 클래스\n",
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.current_level = \"난이도 하\"\n",
        "        self.correct_count = 0\n",
        "        self.incorrect_count = 0\n",
        "        self.current_question = None\n",
        "        self.current_answer = None\n",
        "        self.question_ids = set()\n",
        "\n",
        "    @property\n",
        "    def total_attempts(self):\n",
        "        return self.correct_count + self.incorrect_count\n",
        "\n",
        "    @property\n",
        "    def success_rate(self):\n",
        "        return (self.correct_count / self.total_attempts) if self.total_attempts else 0\n",
        "\n",
        "    def update_stats(self, is_correct: bool):\n",
        "        if is_correct:\n",
        "            self.correct_count += 1\n",
        "            if self.current_level == \"난이도 하\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"난이도 중\"\n",
        "            elif self.current_level == \"난이도 중\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"난이도 상\"\n",
        "        else:\n",
        "            self.incorrect_count += 1\n",
        "            if self.current_level == \"난이도 상\" and self.success_rate < 0.5:\n",
        "                self.current_level = \"난이도 중\"\n",
        "            elif self.current_level == \"난이도 중\" and self.success_rate < 0.3:\n",
        "                self.current_level = \"난이도 하\"\n",
        "\n",
        "# 의학 문제를 처리하기 위한 커스텀 텍스트 분할기\n",
        "class MedicalQuestionSplitter(TextSplitter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pattern = r'(난이도 하 \\d+\\.|난이도 중 \\d+\\.|난이도 상 \\d+\\.)'\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        splits = re.split(self.pattern, text)\n",
        "        chunks = []\n",
        "        for i in range(1, len(splits), 2):\n",
        "            if i+1 < len(splits):\n",
        "                chunk = splits[i] + splits[i+1]\n",
        "                chunks.append(chunk.strip())\n",
        "        return chunks\n",
        "\n",
        "# 로드된 FAISS 인덱스와 호환되는 커스텀 FakeEmbeddings\n",
        "class CustomFakeEmbeddings(FakeEmbeddings):\n",
        "    def __init__(self, size: int = 1024):\n",
        "        super().__init__(size=size)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [np.zeros(self.size) for _ in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return np.zeros(self.size)\n",
        "\n",
        "# 벡터스토어 및 문제 문서 로드\n",
        "def load_vectorstore():\n",
        "    print(\"FAISS 인덱스와 청크 매핑을 로드하는 중...\")\n",
        "    index = faiss.read_index(faiss_path)\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        question_chunks = pickle.load(f)\n",
        "    documents = [Document(page_content=chunk, metadata={\"level\": get_level_from_text(chunk)}) for chunk in question_chunks]\n",
        "    embeddings = CustomFakeEmbeddings(size=index.d)\n",
        "    vectorstore = LangchainFAISS(\n",
        "        embedding_function=embeddings,\n",
        "        index=index,\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={}\n",
        "    )\n",
        "    vectorstore.docstore._dict = {str(i): doc for i, doc in enumerate(documents)}\n",
        "    return vectorstore, documents\n",
        "\n",
        "# 유틸리티 함수들\n",
        "def get_level_from_text(text: str) -> str:\n",
        "    if text.startswith(\"난이도 하\"): return \"난이도 하\"\n",
        "    if text.startswith(\"난이도 중\"): return \"난이도 중\"\n",
        "    if text.startswith(\"난이도 상\"): return \"난이도 상\"\n",
        "    match = re.match(r'(난이도\\s*[하중상])', text)\n",
        "    return match.group(1) if match else \"난이도 하\"\n",
        "\n",
        "def process_question(text: str) -> tuple:\n",
        "    if \"정답:\" not in text:\n",
        "        return text, \"\", \"\"\n",
        "    q_part = text.split(\"정답:\")[0].strip()\n",
        "    a_full = text.split(\"정답:\")[1].split(\"\\n\")[0].strip()\n",
        "    a_part = a_full.split('.')[0].strip() if '.' in a_full else a_full\n",
        "    exp = text.split(\"해설:\")[1].strip() if \"해설:\" in text else \"\"\n",
        "    return q_part, a_part, exp\n",
        "\n",
        "def hide_answer_from_question(text: str) -> str:\n",
        "    return text.split(\"정답:\")[0].strip() if \"정답:\" in text else text\n",
        "\n",
        "# LLM 및 RAG 설정\n",
        "def setup_rag_chain():\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "    vectorstore, documents = load_vectorstore()\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    return llm, retriever, documents\n",
        "\n",
        "# 문제 선택 리트리버 생성\n",
        "def create_question_selector(documents: List[Document], session: SessionState):\n",
        "    def filter_by_level_and_seen(query: str):\n",
        "        level = session.current_level\n",
        "        filtered_docs = [\n",
        "            doc for doc in documents\n",
        "            if doc.metadata.get(\"level\") == level and hash(doc.page_content) not in session.question_ids\n",
        "        ]\n",
        "        if not filtered_docs:\n",
        "            for lvl in [\"난이도 하\", \"난이도 중\", \"난이도 상\"]:\n",
        "                if lvl != level:\n",
        "                    filtered_docs = [\n",
        "                        doc for doc in documents\n",
        "                        if doc.metadata.get(\"level\") == lvl and hash(doc.page_content) not in session.question_ids\n",
        "                    ]\n",
        "                    if filtered_docs:\n",
        "                        session.current_level = lvl\n",
        "                        break\n",
        "            if not filtered_docs:\n",
        "                session.question_ids.clear()\n",
        "                filtered_docs = [doc for doc in documents if doc.metadata.get(\"level\") == session.current_level]\n",
        "        if filtered_docs:\n",
        "            selected = np.random.choice(filtered_docs)\n",
        "            session.question_ids.add(hash(selected.page_content))\n",
        "            return [selected]\n",
        "        return []\n",
        "    class CustomRetriever:\n",
        "        def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            return filter_by_level_and_seen(query)\n",
        "    return CustomRetriever()\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "question_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "다음은 작업치료사 국가고시 문제입니다:\n",
        "\n",
        "{question}\n",
        "\n",
        "이 문제를 학생에게 출제하되, 정답과 해설은 포함하지 마세요.\n",
        "\"\"\")\n",
        "\n",
        "feedback_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 작업치료사 국가고시 전문가입니다.\n",
        "     학생의 답변을 평가하고 유익한 피드백을 제공해주세요.\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    문제: {question}\n",
        "    학생 답변: {user_answer}\n",
        "    정답: {correct_answer}\n",
        "    정답 여부: {is_correct}\n",
        "    해설: {explanation}\n",
        "\n",
        "    다음 지침에 따라 피드백을 제공해주세요:\n",
        "    1. 학생의 이해도를 격려하세요\n",
        "    2. 왜 해당 답변이 맞거나 틀렸는지 간략히 설명하세요\n",
        "    3. 핵심 개념을 한 문장으로 요약하세요\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "# 피드백 체인 생성 함수 (전역 객체로 캐싱)\n",
        "def get_feedback_chain():\n",
        "    if not hasattr(get_feedback_chain, \"chain\"):\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "        get_feedback_chain.chain = (\n",
        "            RunnablePassthrough()\n",
        "            | feedback_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    return get_feedback_chain.chain\n",
        "\n",
        "# 메인 애플리케이션 함수\n",
        "def select_next_question(session: SessionState, question_selector) -> Optional[Document]:\n",
        "    docs = question_selector.get_relevant_documents(\"next_question\")\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "def prepare_next_question(session: SessionState, question_selector) -> str:\n",
        "    next_doc = select_next_question(session, question_selector)\n",
        "    if not next_doc:\n",
        "        return \"**모든 문제를 풀었습니다! 수고하셨습니다.**\"\n",
        "    q, a, exp = process_question(next_doc.page_content)\n",
        "    session.current_question = next_doc.page_content\n",
        "    session.current_answer = a\n",
        "    q_only = hide_answer_from_question(q)\n",
        "    info = (f\"**학습 진행 상황**\\n\"\n",
        "            f\"- 현재 난이도: {session.current_level}\\n\"\n",
        "            f\"- 푼 문제: {session.total_attempts}문제\\n\"\n",
        "            f\"- 맞춘 문제: {session.correct_count}문제\\n\"\n",
        "            f\"- 틀린 문제: {session.incorrect_count}문제\\n\\n\")\n",
        "    return info + f\"**다음 문제**\\n{q_only}\"\n",
        "\n",
        "def generate_feedback(session: SessionState, user_answer: str, is_correct: bool) -> str:\n",
        "    try:\n",
        "        question, _, exp = process_question(session.current_question)\n",
        "        feedback_input = {\n",
        "            \"question\": question,\n",
        "            \"user_answer\": user_answer,\n",
        "            \"correct_answer\": session.current_answer,\n",
        "            \"is_correct\": \"정답입니다!\" if is_correct else \"오답입니다.\",\n",
        "            \"explanation\": exp if exp else \"추가 해설 없음\"\n",
        "        }\n",
        "        fb = get_feedback_chain().invoke(feedback_input)\n",
        "    except Exception as e:\n",
        "        print(f\"피드백 생성 중 오류 발생: {e}\")\n",
        "        fb = (\"정답입니다!\" if is_correct else \"오답입니다.\") + f\" 정답은 {session.current_answer}입니다.\"\n",
        "    if not is_correct:\n",
        "        fb += f\"\\n\\n**정답**: {session.current_answer}\"\n",
        "    _, _, exp = process_question(session.current_question)\n",
        "    if exp:\n",
        "        fb += f\"\\n\\n**해설**: {exp}\"\n",
        "    return fb\n",
        "\n",
        "# Gradio UI 함수\n",
        "def chat_interface(message, history, session_state, question_selector):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "    if not history:\n",
        "        doc = select_next_question(session_state, question_selector)\n",
        "        if not doc:\n",
        "            return [{\"role\": \"assistant\", \"content\": \"문제를 불러올 수 없습니다.\"}], \"\"\n",
        "        q, a, _ = process_question(doc.page_content)\n",
        "        session_state.current_question = doc.page_content\n",
        "        session_state.current_answer = a\n",
        "        return [\n",
        "            {\"role\": \"assistant\", \"content\": \"안녕하세요! 작업치료사 국가고시 문제 풀이를 시작합니다.\"},\n",
        "            {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "        ], \"\"\n",
        "    new_hist = history.copy()\n",
        "    user_ans = message.strip().upper()\n",
        "    new_hist.append({\"role\": \"user\", \"content\": user_ans})\n",
        "    correct = (user_ans == session_state.current_answer.upper())\n",
        "    session_state.update_stats(correct)\n",
        "    fb = generate_feedback(session_state, user_ans, correct)\n",
        "    nxt_q = prepare_next_question(session_state, question_selector)\n",
        "    new_hist.append({\"role\": \"assistant\", \"content\": fb + \"\\n\\n\" + nxt_q})\n",
        "    return new_hist, \"\"\n",
        "\n",
        "def answer_click(answer, history, session_state, question_selector):\n",
        "    new_history, _ = chat_interface(answer, history, session_state, question_selector)\n",
        "    return new_history\n",
        "\n",
        "def clear_chat(session_state, question_selector):\n",
        "    new_sess = SessionState()\n",
        "    first_doc = select_next_question(new_sess, question_selector)\n",
        "    if not first_doc:\n",
        "        return new_sess, [{\"role\": \"assistant\", \"content\": \"문제를 불러올 수 없습니다.\"}]\n",
        "    q, a, _ = process_question(first_doc.page_content)\n",
        "    new_sess.current_question = first_doc.page_content\n",
        "    new_sess.current_answer = a\n",
        "    return new_sess, [\n",
        "        {\"role\": \"assistant\", \"content\": \"안녕하세요! 작업치료사 국가고시 문제 풀이를 시작합니다.\"},\n",
        "        {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "    ]\n",
        "\n",
        "# 메인 애플리케이션\n",
        "def main():\n",
        "    llm, retriever, documents = setup_rag_chain()\n",
        "    session_state = SessionState()\n",
        "    question_selector = create_question_selector(documents, session_state)\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "        .gradio-container {max-width: 900px; margin: auto;}\n",
        "        .chat-history {height: 600px !important;}\n",
        "        footer {display: none !important;}\n",
        "    \"\"\") as demo:\n",
        "        gr.Markdown(\"# 🎓 작업치료사 국가고시 문제 학습 챗봇\")\n",
        "        gr.Markdown(\"버튼을 눌러 답안을 선택하세요.\")\n",
        "        session_state_gr = gr.State(session_state)\n",
        "        question_selector_gr = gr.State(question_selector)\n",
        "        chatbot = gr.Chatbot(label=\"학습 대화\", height=500, type=\"messages\", render_markdown=True)\n",
        "        with gr.Row():\n",
        "            btn_A = gr.Button(\"A\")\n",
        "            btn_B = gr.Button(\"B\")\n",
        "            btn_C = gr.Button(\"C\")\n",
        "            btn_D = gr.Button(\"D\")\n",
        "            clear_btn = gr.Button(\"초기화\")\n",
        "        btn_A.click(\n",
        "            lambda history, ss, qs: answer_click(\"A\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_B.click(\n",
        "            lambda history, ss, qs: answer_click(\"B\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_C.click(\n",
        "            lambda history, ss, qs: answer_click(\"C\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_D.click(\n",
        "            lambda history, ss, qs: answer_click(\"D\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        clear_btn.click(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "        demo.load(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "    demo.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6KXSVCOS-7l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}