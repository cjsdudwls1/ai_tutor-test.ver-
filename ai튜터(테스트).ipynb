{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1QaCSoYhP2FfXGE_VkcImuQTyKFC1G-Hk",
      "authorship_tag": "ABX9TyMOaZ/s983Zg3px5X4pQDS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjsdudwls1/ai_tutor-test.ver-/blob/main/ai%ED%8A%9C%ED%84%B0(%ED%85%8C%EC%8A%A4%ED%8A%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFpz_H5dEqQX",
        "outputId": "f27c1e87-876b-4643-c08e-93f5a6798a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ë²„ì „ ëª…ì‹œ)\n",
        "!pip install faiss-cpu langchain-text-splitters python-docx unstructured langchain_community llama-index gradio docx2txt -qqq\n",
        "!pip install --upgrade \"google-genai>=1.7.0\" \"langchain-google-genai>=2.1.3\" -qqq\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tDJ_m7Aq6XN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1ë‹¨ê³„ : ë¬¸ì„œ ë¡œë“œ**"
      ],
      "metadata": {
        "id": "mc4Z5XeFj3rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "\n",
        "loader = Docx2txtLoader(\"/content/drive/MyDrive/(á„‚á…¡á†«á„‹á…µá„ƒá…©á„‘á…­á„‰á…µá„‡á…©á†«) á„‰á…¢á†·á„‘á…³á†¯á„†á…®á†«á„Œá…¦ á„‹á…¯á„ƒá…³á„…á…© á„ƒá…¡á„‰á…µ.docx\")  # ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”\n",
        "\n",
        "docs = loader.load()  # ë¬¸ì„œ ë¡œë”©\n",
        "\n",
        "print(len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "demLNfoK4SEv",
        "outputId": "662a417e-c693-4500-e5c8-0e2004d1188f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llmì´ ëª»ì•Œì•„ ë“¤ì„ ìˆ˜ ìˆìœ¼ë‹ˆ <br>**ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ 'í•˜ N.', 'ì¤‘ N.', 'ìƒ N.'ì„ 'ë‚œì´ë„ í•˜ N.', 'ë‚œì´ë„ ì¤‘ N.', 'ë‚œì´ë„ ìƒ N.'ìœ¼ë¡œ ë³€í™˜**"
      ],
      "metadata": {
        "id": "EtcOc1UR0gI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def transform_difficulty_markers(docs):\n",
        "    for i, doc in enumerate(docs):\n",
        "        modified_content = re.sub(r'(í•˜|ì¤‘|ìƒ)( \\d+\\.)', r'ë‚œì´ë„ \\1\\2', doc.page_content)\n",
        "        docs[i].page_content = modified_content\n",
        "    return docs\n",
        "\n",
        "# ë³€í™˜ëœ ë¬¸ì„œ ì‚¬ìš©\n",
        "docs = transform_difficulty_markers(docs)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv2ry88STBCR",
        "outputId": "81a350f6-3bfa-47fe-c104-3e4c08f60f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìƒ˜í”Œë¬¸ì œ (ì •ë‹µ ë° í•´ì„¤ í¬í•¨)\n",
            "\n",
            "ë‚œì´ë„ í•˜ 1. ë‹¤ìŒ ì¤‘ ê°„(liver)ì—ì„œ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€?\n",
            "\n",
            "A. ì•Œë¶€ë¯¼(albumin) í•©ì„±\n",
            "\n",
            "B. ë‹´ì¦™(bile) ìƒì„±\n",
            "\n",
            "C. í˜ˆë‹¹ ì¡°ì ˆ(glucose regulation)\n",
            "\n",
            "D. ì¸ìŠë¦°(insulin) ë¶„ë¹„\n",
            "\n",
            "ì •ë‹µ: D. ì¸ìŠë¦°(insulin) ë¶„ë¹„\n",
            " í•´ì„¤: ì¸ìŠë¦°ì€ ì·Œì¥(ì´ì)ì—ì„œ ë¶„ë¹„ë˜ë©°, ê°„ì€ ì•Œë¶€ë¯¼ í•©ì„±, ë‹´ì¦™ ìƒì„±, í˜ˆë‹¹ ì¡°ì ˆì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 2. ê·¼ìˆ˜ì¶•(muscle contraction)ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ì—¬í•˜ëŠ” ë‹¨ë°±ì§ˆ ë³µí•©ì²´ë¡œ ì˜³ì€ ê²ƒì€?\n",
            "\n",
            "A. ë¯¸ì˜¤ì‹ (myosin)ê³¼ íŠ¸ë¡œí¬ë‹Œ(troponin)\n",
            "\n",
            "B. íŠ¸ë¡œí¬ë‹Œ(troponin)ê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ (tropomyosin)\n",
            "\n",
            "C. ì•¡í‹´(actin)ê³¼ ì½œë¼ê²(collagen)\n",
            "\n",
            "D. ì—˜ë¼ìŠ¤í‹´(elastin)ê³¼ ë¯¸ì˜¤ì‹ (myosin)\n",
            "\n",
            "ì •ë‹µ: B. íŠ¸ë¡œí¬ë‹Œ(troponin)ê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ (tropomyosin)\n",
            " í•´ì„¤: íŠ¸ë¡œí¬ë‹Œê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ ì€ ê·¼ìˆ˜ì¶• ì‹œ ì¹¼ìŠ˜ê³¼ ê²°í•©í•˜ì—¬ ì•¡í‹´-ë¯¸ì˜¤ì‹  ìƒí˜¸ì‘ìš©ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ í•˜ 3. ë‹¤ìŒ ì¤‘ í˜ˆì•¡-ë‡Œ ì¥ë²½(blood-brain barrier)ì„ í†µê³¼í•˜ê¸° ê°€ì¥ ì‰¬ìš´ ë¬¼ì§ˆì€?\n",
            "\n",
            "A. í¬ë„ë‹¹(glucose)\n",
            "\n",
            "B. ì‚°ì†Œ(oxygen)\n",
            "\n",
            "C. ë‹¨ë°±ì§ˆ(protein)\n",
            "\n",
            "D. ë‚˜íŠ¸ë¥¨ ì´ì˜¨(sodium ion)\n",
            "\n",
            "ì •ë‹µ: B. ì‚°ì†Œ(oxygen)\n",
            " í•´ì„¤: ì‚°ì†ŒëŠ” ì§€ìš©ì„± ë¶„ìë¡œ, í˜ˆì•¡-ë‡Œ ì¥ë²½ì„ ê°€ì¥ ì‰½ê²Œ í†µê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 4. ì‹ ì¥ì—ì„œ ì—¬ê³¼(filtration)ê°€ ì¼ì–´ë‚˜ëŠ” ì£¼ìš” êµ¬ì¡°ë¬¼ì€?\n",
            "\n",
            "A. ë³´ìš°ë§Œì£¼ë¨¸ë‹ˆ(Bowman's capsule)\n",
            "\n",
            "B. ê·¼ìœ„ì„¸ë‡¨ê´€(proximal convoluted tubule)\n",
            "\n",
            "C. í—¨ë ˆê³ ë¦¬(loop of Henle)\n",
            "\n",
            "D. ì§‘í•©ê´€(collecting duct)\n",
            "\n",
            "ì •ë‹µ: A. ë³´ìš°ë§Œì£¼ë¨¸ë‹ˆ(Bowman's capsule)\n",
            " í•´ì„¤: ì‹ ì¥ì—ì„œ í˜ˆì•¡ì˜ ì—¬ê³¼ëŠ” ì‚¬êµ¬ì²´ì™€ ë³´ìš°ë§Œì£¼ë¨¸ë‹ˆì—ì„œ ì¼ì–´ë‚©ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 5. ë‹¤ìŒ ì¤‘ ììœ¨ì‹ ê²½ê³„(autonomic nervous system)ì˜ êµê°ì‹ ê²½ê³„(sympathetic nervous system)ì˜ ë°˜ì‘ì´ ì•„ë‹Œ ê²ƒì€?\n",
            "\n",
            "A. ë™ê³µ í™•ëŒ€(pupil dilation)\n",
            "\n",
            "B. ì‹¬ë°•ìˆ˜ ì¦ê°€(increased heart rate)\n",
            "\n",
            "C. ì†Œí™”í™œë™ ì´‰ì§„(enhanced digestion)\n",
            "\n",
            "D. ê¸°ê´€ì§€ í™•ì¥(bronchodilation)\n",
            "\n",
            "ì •ë‹µ: C. ì†Œí™”í™œë™ ì´‰ì§„(enhanced digestion)\n",
            " í•´ì„¤: êµê°ì‹ ê²½ê³„ëŠ” ì†Œí™” í™œë™ì„ ì–µì œí•˜ê³ , ë¶€êµê°ì‹ ê²½ê³„ê°€ ì†Œí™”í™œë™ì„ ì´‰ì§„í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ìƒ 6. í˜¸ë¥´ëª¬ ì¡°ì ˆ(hormonal regulation)ì— ìˆì–´ ìŒì„±ë˜ë¨¹ì„(negative feedback)ì˜ ì˜ˆë¡œ ì ì ˆí•œ ê²ƒì€?\n",
            "\n",
            "A. í˜ˆë‹¹ ì¦ê°€ ì‹œ ê¸€ë£¨ì¹´ê³¤(glucagon) ë¶„ë¹„ ì¦ê°€\n",
            "\n",
            "B. í˜ˆì¤‘ ì¹¼ìŠ˜ ë†ë„ ê°ì†Œ ì‹œ ì¹¼ì‹œí† ë‹Œ(calcitonin) ë¶„ë¹„ ì¦ê°€\n",
            "\n",
            "C. ê°‘ìƒì„  í˜¸ë¥´ëª¬(thyroid hormone) ì¦ê°€ ì‹œ TSH ê°ì†Œ\n",
            "\n",
            "D. ì²´ì˜¨ ì¦ê°€ ì‹œ ë•€ìƒ˜ ë¶„ë¹„ ê°ì†Œ\n",
            "\n",
            "ì •ë‹µ: C. ê°‘ìƒì„  í˜¸ë¥´ëª¬(thyroid hormone) ì¦ê°€ ì‹œ TSH ê°ì†Œ\n",
            " í•´ì„¤: ê°‘ìƒì„  í˜¸ë¥´ëª¬ì´ ì¦ê°€í•˜ë©´ ë‡Œí•˜ìˆ˜ì²´ì—ì„œ TSH ë¶„ë¹„ê°€ ì–µì œë˜ëŠ” ê²ƒì´ ëŒ€í‘œì ì¸ ìŒì„± í”¼ë“œë°±ì…ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 7. ê·¼ìœ¡ì˜ ìœ í˜• ì¤‘ ì‹¬ì¥ê·¼(cardiac muscle)ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€?\n",
            "\n",
            "A. ë¶ˆìˆ˜ì˜ê·¼(involuntary muscle)ì´ë‹¤.\n",
            "\n",
            "B. ê°„ê·¹ê²°í•©(gap junction)ì„ í¬í•¨í•œë‹¤.\n",
            "\n",
            "C. ìˆ˜ì¶•ì€ ììœ¨ì‹ ê²½ê³„ì— ì˜í•´ ì¡°ì ˆëœë‹¤.\n",
            "\n",
            "D. í•µì´ ë‹¤ìˆ˜ ì¡´ì¬í•œë‹¤.\n",
            "\n",
            "ì •ë‹µ: D. í•µì´ ë‹¤ìˆ˜ ì¡´ì¬í•œë‹¤.\n",
            " í•´ì„¤: ì‹¬ì¥ê·¼ì€ ëŒ€ë¶€ë¶„ ë‹¨ì¼ í•µì„ ê°€ì§€ë©° ë‹¤ìˆ˜ì˜ í•µì€ ê³¨ê²©ê·¼ì˜ íŠ¹ì§•ì…ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ í•˜ 8. í˜ˆì•¡ì˜ í•­ìƒì„±(homeostasis)ì„ ìœ ì§€í•˜ëŠ” ê³¼ì •ì—ì„œ í˜ˆì†ŒíŒ(platelets)ì˜ ì£¼ìš” ì—­í• ì€?\n",
            "\n",
            "A. ì‚°ì†Œ ìš´ë°˜\n",
            "\n",
            "B. ë©´ì—­ ë°˜ì‘ ì´‰ì§„\n",
            "\n",
            "C. í˜ˆì•¡ ì‘ê³  ì‹œì‘\n",
            "\n",
            "D. ì‚¼íˆ¬ì•• ìœ ì§€\n",
            "\n",
            "ì •ë‹µ: C. í˜ˆì•¡ ì‘ê³  ì‹œì‘\n",
            " í•´ì„¤: í˜ˆì†ŒíŒì€ í˜ˆê´€ì´ ì†ìƒë˜ì—ˆì„ ë•Œ í˜ˆì•¡ì‘ê³ ë¥¼ ì‹œì‘í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 9. ì‹ ê²½ì„¸í¬(neuron)ì˜ í™œë™ì „ìœ„(action potential)ì—ì„œ íƒˆë¶„ê·¹(depolarization)ì´ ì¼ì–´ë‚˜ëŠ” ì›ì¸ì€?\n",
            "\n",
            "A. ì¹¼ìŠ˜ ì´ì˜¨ì˜ ìœ ì…\n",
            "\n",
            "B. ì¹¼ë¥¨ ì´ì˜¨ì˜ ìœ ì¶œ\n",
            "\n",
            "C. ë‚˜íŠ¸ë¥¨ ì´ì˜¨ì˜ ìœ ì…\n",
            "\n",
            "D. ì—¼ì†Œ ì´ì˜¨ì˜ ìœ ì…\n",
            "\n",
            "ì •ë‹µ: C. ë‚˜íŠ¸ë¥¨ ì´ì˜¨ì˜ ìœ ì…\n",
            " í•´ì„¤: ë‚˜íŠ¸ë¥¨ ì´ì˜¨ì´ ì„¸í¬ ì•ˆìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ì„œ ë§‰ ì „ìœ„ê°€ ì–‘ì „í•˜ë¡œ ë³€í•˜ëŠ” ê²ƒì´ íƒˆë¶„ê·¹ì…ë‹ˆë‹¤.\n",
            "\n",
            "ë‚œì´ë„ ì¤‘ 10. ë‹¤ìŒ ì¤‘ ê°„ë‡Œ(diencephalon)ì— ì†í•˜ëŠ” êµ¬ì¡°ë¬¼ì´ ì•„ë‹Œ ê²ƒì€?\n",
            "\n",
            "A. ì‹œìƒ(thalamus)\n",
            "\n",
            "B. ì‹œìƒí•˜ë¶€(hypothalamus)\n",
            "\n",
            "C. ì†¡ê³¼ì„ (pineal gland)\n",
            "\n",
            "D. ì—°ìˆ˜(medulla oblongata)\n",
            "\n",
            "ì •ë‹µ: D. ì—°ìˆ˜(medulla oblongata)\n",
            " í•´ì„¤: ì—°ìˆ˜ëŠ” ë‡Œê°„ì— ì†í•˜ë©°, ê°„ë‡Œì—ëŠ” ì‹œìƒ, ì‹œìƒí•˜ë¶€, ì†¡ê³¼ì„ ì´ í¬í•¨ë©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¶„í• **"
      ],
      "metadata": {
        "id": "8yUwnpBuQqRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "\n",
        "# 2. ì²­í‚¹ì„ ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ ì •ì˜\n",
        "pattern = r'(ë‚œì´ë„ í•˜ \\d+\\.|ë‚œì´ë„ ì¤‘ \\d+\\.|ë‚œì´ë„ ìƒ \\d+\\.)'\n",
        "\n",
        "# 3. ì»¤ìŠ¤í…€ ë¶„í•  í•¨ìˆ˜\n",
        "def question_splitter(text, pattern):\n",
        "    splits = re.split(pattern, text)\n",
        "    chunks = []\n",
        "    for i in range(1, len(splits), 2):\n",
        "        if i+1 < len(splits):\n",
        "            chunk = splits[i] + splits[i+1]\n",
        "            chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# 4. ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ë° ë¶„í• \n",
        "full_text = docs[0].page_content  # ë¡œë“œëœ ë¬¸ì„œì˜ ì „ì²´ í…ìŠ¤íŠ¸\n",
        "question_chunks = question_splitter(full_text, pattern)\n",
        "\n",
        "# 5. ê²°ê³¼ í™•ì¸\n",
        "for idx, chunk in enumerate(question_chunks[:3]):  # ìƒìœ„ 3ê°œ ìƒ˜í”Œ ì¶œë ¥\n",
        "    print(f\"=== ë¬¸ì œ {idx+1} ===\")\n",
        "    print(chunk)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c05YqO-4SMB",
        "outputId": "34608a77-461f-4990-9e93-2f21d3a85be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ë¬¸ì œ 1 ===\n",
            "ë‚œì´ë„ í•˜ 1. ë‹¤ìŒ ì¤‘ ê°„(liver)ì—ì„œ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€?\n",
            "\n",
            "A. ì•Œë¶€ë¯¼(albumin) í•©ì„±\n",
            "\n",
            "B. ë‹´ì¦™(bile) ìƒì„±\n",
            "\n",
            "C. í˜ˆë‹¹ ì¡°ì ˆ(glucose regulation)\n",
            "\n",
            "D. ì¸ìŠë¦°(insulin) ë¶„ë¹„\n",
            "\n",
            "ì •ë‹µ: D. ì¸ìŠë¦°(insulin) ë¶„ë¹„\n",
            " í•´ì„¤: ì¸ìŠë¦°ì€ ì·Œì¥(ì´ì)ì—ì„œ ë¶„ë¹„ë˜ë©°, ê°„ì€ ì•Œë¶€ë¯¼ í•©ì„±, ë‹´ì¦™ ìƒì„±, í˜ˆë‹¹ ì¡°ì ˆì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "=== ë¬¸ì œ 2 ===\n",
            "ë‚œì´ë„ ì¤‘ 2. ê·¼ìˆ˜ì¶•(muscle contraction)ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ì—¬í•˜ëŠ” ë‹¨ë°±ì§ˆ ë³µí•©ì²´ë¡œ ì˜³ì€ ê²ƒì€?\n",
            "\n",
            "A. ë¯¸ì˜¤ì‹ (myosin)ê³¼ íŠ¸ë¡œí¬ë‹Œ(troponin)\n",
            "\n",
            "B. íŠ¸ë¡œí¬ë‹Œ(troponin)ê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ (tropomyosin)\n",
            "\n",
            "C. ì•¡í‹´(actin)ê³¼ ì½œë¼ê²(collagen)\n",
            "\n",
            "D. ì—˜ë¼ìŠ¤í‹´(elastin)ê³¼ ë¯¸ì˜¤ì‹ (myosin)\n",
            "\n",
            "ì •ë‹µ: B. íŠ¸ë¡œí¬ë‹Œ(troponin)ê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ (tropomyosin)\n",
            " í•´ì„¤: íŠ¸ë¡œí¬ë‹Œê³¼ íŠ¸ë¡œí¬ë§ˆì´ì˜¤ì‹ ì€ ê·¼ìˆ˜ì¶• ì‹œ ì¹¼ìŠ˜ê³¼ ê²°í•©í•˜ì—¬ ì•¡í‹´-ë¯¸ì˜¤ì‹  ìƒí˜¸ì‘ìš©ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "=== ë¬¸ì œ 3 ===\n",
            "ë‚œì´ë„ í•˜ 3. ë‹¤ìŒ ì¤‘ í˜ˆì•¡-ë‡Œ ì¥ë²½(blood-brain barrier)ì„ í†µê³¼í•˜ê¸° ê°€ì¥ ì‰¬ìš´ ë¬¼ì§ˆì€?\n",
            "\n",
            "A. í¬ë„ë‹¹(glucose)\n",
            "\n",
            "B. ì‚°ì†Œ(oxygen)\n",
            "\n",
            "C. ë‹¨ë°±ì§ˆ(protein)\n",
            "\n",
            "D. ë‚˜íŠ¸ë¥¨ ì´ì˜¨(sodium ion)\n",
            "\n",
            "ì •ë‹µ: B. ì‚°ì†Œ(oxygen)\n",
            " í•´ì„¤: ì‚°ì†ŒëŠ” ì§€ìš©ì„± ë¶„ìë¡œ, í˜ˆì•¡-ë‡Œ ì¥ë²½ì„ ê°€ì¥ ì‰½ê²Œ í†µê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3ë‹¨ê³„: ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "jvkDZ65_7mm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = 'BM-K/KoSimCSE-roberta'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ ìˆ˜ì • (ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”©)\n",
        "def get_embeddings(sentences):\n",
        "    # ì…ë ¥: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [\"ë¬¸ì¥1\", \"ë¬¸ì¥2\", ...])\n",
        "    inputs = tokenizer(\n",
        "        sentences,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,  # ìµœëŒ€ ê¸¸ì´ ì œí•œ\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ (ì²« ë²ˆì§¸ í† í°)\n",
        "    # outputs.last_hidden_state.shape = (batch_size, seq_len, hidden_dim)\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # ì •ê·œí™” (ì„ íƒ ì‚¬í•­)\n",
        "    cls_embeddings = torch.nn.functional.normalize(cls_embeddings, p=2, dim=1)\n",
        "\n",
        "    return cls_embeddings\n",
        "\n",
        "# 3. ì˜ˆì‹œ: ì²­í‚¹ëœ ë¬¸ì œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©\n",
        "embeddings = get_embeddings(question_chunks)  # (num_chunks, 768) í˜•íƒœ í…ì„œ\n",
        "\n",
        "# 4. ì„ë² ë”© ê²°ê³¼ í™•ì¸\n",
        "print(embeddings.shape)  # â†’ torch.Size([ë¬¸í•­ê°œìˆ˜, 768])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJWp-tl4SQw",
        "outputId": "cc8af081-f3fc-4ed4-d60e-f17c661c1242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4ë‹¨ê³„: FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±**"
      ],
      "metadata": {
        "id": "evf_f6Lh_HY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "save_dir = '/content/drive/MyDrive/tutor_store'\n",
        "\n",
        "# 1. ì„ë² ë”©ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜ (KoSimCSE ì¶œë ¥ì´ PyTorch í…ì„œë¼ ê°€ì •)\n",
        "embeddings_np = embeddings.numpy().astype('float32')  # (num_chunks, 768)\n",
        "\n",
        "# 2. FAISS ì¸ë±ìŠ¤ ìƒì„± (L2 ê±°ë¦¬ ê¸°ì¤€)\n",
        "dimension = embeddings_np.shape[1]  # 768\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# 3. ì„ë² ë”©ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€\n",
        "faiss_index.add(embeddings_np)\n",
        "\n",
        "# 4. ë²¡í„°ìŠ¤í† ì–´ì™€ ì²­í¬ ë§¤í•‘ ì €ì¥\n",
        "faiss_path = os.path.join(save_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(save_dir, \"chunk_mapping.pkl\")\n",
        "\n",
        "# FAISS ì¸ë±ìŠ¤ ì €ì¥\n",
        "faiss.write_index(faiss_index, faiss_path)\n",
        "print(f\"FAISS ì¸ë±ìŠ¤ê°€ {faiss_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ì²­í¬ ë¦¬ìŠ¤íŠ¸ ì €ì¥ (ê²€ìƒ‰ ì‹œ í…ìŠ¤íŠ¸ ë³µì›ìš©)\n",
        "with open(chunk_path, 'wb') as f:\n",
        "    pickle.dump(question_chunks, f)\n",
        "print(f\"ì²­í¬ ë§¤í•‘ì´ {chunk_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOn1UZpw4SXX",
        "outputId": "888262c3-755d-4541-cbfe-90ddea502fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS ì¸ë±ìŠ¤ê°€ /content/drive/MyDrive/tutor_store/medical_qa_index.faissì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "ì²­í¬ ë§¤í•‘ì´ /content/drive/MyDrive/tutor_store/chunk_mapping.pklì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5ë‹¨ê³„: tutor ì‹œìŠ¤í…œ êµ¬í˜„ (gemini-1.5-pro)"
      ],
      "metadata": {
        "id": "EAuwqYyvWbO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ê¸°ë³¸ ì„¤ì •**"
      ],
      "metadata": {
        "id": "kaTOxXlE7IwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import gradio as gr\n",
        "from google.colab import drive, userdata\n",
        "from typing import List, Dict, Optional, Any\n",
        "from langchain_community.vectorstores.faiss import FAISS as LangchainFAISS\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import FakeEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "\n",
        "# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° API í‚¤ ì„¤ì •\n",
        "drive.mount('/content/drive')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('Gemini_API_KEY')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lRVxIZC0FE9",
        "outputId": "f42bd2f7-2771-462d-bc8e-3453acbae1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ ì„¤ì •**"
      ],
      "metadata": {
        "id": "DSyaObKd7N8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_dir = '/content/drive/MyDrive/tutor_store'\n",
        "faiss_path = os.path.join(store_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(store_dir, \"chunk_mapping.pkl\")"
      ],
      "metadata": {
        "id": "9Q2kMYpY7JrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. SessionState í´ë˜ìŠ¤ëŠ” ì‚¬ìš©ìì˜ ë‚œì´ë„(current_level), ì •ë‹µ/ì˜¤ë‹µ ê°œìˆ˜, í˜„ì¬ ë¬¸ì œÂ·ì •ë‹µ ë³´ê´€, ì´ë¯¸ ë³¸ ë¬¸ì œ ID ì§‘í•©ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. total_attempts, success_rate í”„ë¡œí¼í‹°ë¡œ ì‹œë„ íšŸìˆ˜ì™€ ì„±ê³µë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. update_stats() ë©”ì„œë“œë¡œ ì •ë‹µ ì—¬ë¶€ì— ë”°ë¼ ì¹´ìš´íŠ¸ë¥¼ ê°±ì‹ í•˜ê³ , ì¼ì • ê¸°ì¤€ ì´ìƒì¼ ë•Œ ë‚œì´ë„ë¥¼ ìƒí–¥/í•˜í–¥ ì¡°ì •í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "jJJTVsD37dmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.current_level = \"ë‚œì´ë„ í•˜\"\n",
        "        self.correct_count = 0\n",
        "        self.incorrect_count = 0\n",
        "        self.current_question = None\n",
        "        self.current_answer = None\n",
        "        self.question_ids = set()\n",
        "\n",
        "    @property\n",
        "    def total_attempts(self):\n",
        "        return self.correct_count + self.incorrect_count\n",
        "\n",
        "    @property\n",
        "    def success_rate(self):\n",
        "        return (self.correct_count / self.total_attempts) if self.total_attempts else 0\n",
        "\n",
        "    def update_stats(self, is_correct: bool):\n",
        "        if is_correct:\n",
        "            self.correct_count += 1\n",
        "            if self.current_level == \"ë‚œì´ë„ í•˜\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"ë‚œì´ë„ ì¤‘\"\n",
        "            elif self.current_level == \"ë‚œì´ë„ ì¤‘\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"ë‚œì´ë„ ìƒ\"\n",
        "        else:\n",
        "            self.incorrect_count += 1\n",
        "            if self.current_level == \"ë‚œì´ë„ ìƒ\" and self.success_rate < 0.5:\n",
        "                self.current_level = \"ë‚œì´ë„ ì¤‘\"\n",
        "            elif self.current_level == \"ë‚œì´ë„ ì¤‘\" and self.success_rate < 0.3:\n",
        "                self.current_level = \"ë‚œì´ë„ í•˜\"\n"
      ],
      "metadata": {
        "id": "WrSh8LCd7XUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **MedicalQuestionSplitter (ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ë¶„í• ê¸°)**\n",
        "1. TextSplitterë¥¼ ìƒì†í•˜ì—¬, â€œë‚œì´ë„ í•˜ 1.â€ì²˜ëŸ¼ ê° ë¬¸ì œ ì œëª© íŒ¨í„´ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. split_text()ëŠ” ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì§ìˆ˜ ì¸ë±ìŠ¤ì˜ ì œëª©ê³¼ ë¬¸ì œ ë³¸ë¬¸ì„ í•©ì³ì„œ ì²­í¬(ë¬¸ì œ ë‹¨ìœ„) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "D-7VQA2u7kj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalQuestionSplitter(TextSplitter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pattern = r'(ë‚œì´ë„ í•˜ \\d+\\.|ë‚œì´ë„ ì¤‘ \\d+\\.|ë‚œì´ë„ ìƒ \\d+\\.)'\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        splits = re.split(self.pattern, text)\n",
        "        chunks = []\n",
        "        for i in range(1, len(splits), 2):\n",
        "            if i+1 < len(splits):\n",
        "                chunk = splits[i] + splits[i+1]\n",
        "                chunks.append(chunk.strip())\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "H36KIbOJ7kn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CustomFakeEmbeddings (ì»¤ìŠ¤í…€ ì„ë² ë”©)**\n",
        "\n",
        "1.\n",
        "FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œ, ì‹¤ì œ ì„ë² ë”© ëŒ€ì‹  ëª¨ë‘ 0ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” FakeEmbeddingsë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. embed_documents(), embed_query() ëª¨ë‘ ê³ ì • ê¸¸ì´ 0ë²¡í„°ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "0pIm8x6I7uaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomFakeEmbeddings(FakeEmbeddings):\n",
        "    def __init__(self, size: int = 1024):\n",
        "        super().__init__(size=size)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [np.zeros(self.size) for _ in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return np.zeros(self.size)"
      ],
      "metadata": {
        "id": "bKTIVq3U7phB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë²¡í„°ìŠ¤í† ì–´ ë° ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜**\n",
        "1. FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì½ì–´ì˜¤ê³ , pickleë¡œ ì €ì¥ëœ ë¬¸ì œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "\n",
        "2. ê° ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë˜í•‘í•˜ë©°, ë ˆë²¨ ì •ë³´ëŠ” get_level_from_text()ë¡œ ë©”íƒ€ë°ì´í„°ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. ì•ì„œ ì •ì˜í•œ CustomFakeEmbeddingsë¥¼ ì‚¬ìš©í•´ LangChain FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•œ í›„, docstoreì— ë¬¸ì„œë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Ykyj6Qof7yvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vectorstore():\n",
        "    print(\"FAISS ì¸ë±ìŠ¤ì™€ ì²­í¬ ë§¤í•‘ì„ ë¡œë“œí•˜ëŠ” ì¤‘...\")\n",
        "    index = faiss.read_index(faiss_path)\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        question_chunks = pickle.load(f)\n",
        "    documents = [Document(page_content=chunk, metadata={\"level\": get_level_from_text(chunk)}) for chunk in question_chunks]\n",
        "    embeddings = CustomFakeEmbeddings(size=index.d)\n",
        "    vectorstore = LangchainFAISS(\n",
        "        embedding_function=embeddings,\n",
        "        index=index,\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={}\n",
        "    )\n",
        "    vectorstore.docstore._dict = {str(i): doc for i, doc in enumerate(documents)}\n",
        "    return vectorstore, documents\n"
      ],
      "metadata": {
        "id": "W1nuAPsb7zAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤**\n",
        "1. get_level_from_text(): í…ìŠ¤íŠ¸ ë§¨ ì•ì˜ â€œë‚œì´ë„â€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "2. process_question(): ë³¸ë¬¸ì—ì„œ ë¬¸ì œ, ì •ë‹µ, í•´ì„¤ì„ ë¶„ë¦¬í•˜ì—¬ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. hide_answer_from_question(): ë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë‹µ ë¶€ë¶„ì„ ì œê±°í•´ í•™ìƒì—ê²Œë§Œ ì§ˆë¬¸ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "cqFRFbcL78p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_level_from_text(text: str) -> str:\n",
        "    if text.startswith(\"ë‚œì´ë„ í•˜\"): return \"ë‚œì´ë„ í•˜\"\n",
        "    if text.startswith(\"ë‚œì´ë„ ì¤‘\"): return \"ë‚œì´ë„ ì¤‘\"\n",
        "    if text.startswith(\"ë‚œì´ë„ ìƒ\"): return \"ë‚œì´ë„ ìƒ\"\n",
        "    match = re.match(r'(ë‚œì´ë„\\s*[í•˜ì¤‘ìƒ])', text)\n",
        "    return match.group(1) if match else \"ë‚œì´ë„ í•˜\"\n",
        "\n",
        "def process_question(text: str) -> tuple:\n",
        "    if \"ì •ë‹µ:\" not in text:\n",
        "        return text, \"\", \"\"\n",
        "    q_part = text.split(\"ì •ë‹µ:\")[0].strip()\n",
        "    a_full = text.split(\"ì •ë‹µ:\")[1].split(\"\\n\")[0].strip()\n",
        "    a_part = a_full.split('.')[0].strip() if '.' in a_full else a_full\n",
        "    exp = text.split(\"í•´ì„¤:\")[1].strip() if \"í•´ì„¤:\" in text else \"\"\n",
        "    return q_part, a_part, exp\n",
        "\n",
        "def hide_answer_from_question(text: str) -> str:\n",
        "    return text.split(\"ì •ë‹µ:\")[0].strip() if \"ì •ë‹µ:\" in text else text\n"
      ],
      "metadata": {
        "id": "Fdifk7vl78w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAG(ê²€ìƒ‰ì—°ê³„ìƒì„±) ì²´ì¸ ë° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •**"
      ],
      "metadata": {
        "id": "2C-opp897__d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_rag_chain():\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "    vectorstore, documents = load_vectorstore()\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    return llm, retriever, documents\n"
      ],
      "metadata": {
        "id": "_rxisxAL8AEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì‚¬ìš©ì ë§ì¶¤ ì§ˆë¬¸ ì„ íƒê¸° ìƒì„±**\n",
        "1. í˜„ì¬ ì„¸ì…˜ì˜ ë‚œì´ë„ì™€ ì´ë¯¸ ë³¸ ë¬¸ì œ ëª©ë¡ì„ ê¸°ì¤€ìœ¼ë¡œ, ì•„ì§ ë³´ì§€ ì•Šì€ ë¬¸ì œë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. í•´ë‹¹ ë‚œì´ë„ì˜ ë¬¸ì œê°€ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë‚œì´ë„ë¡œ ë³€ê²½í•˜ê±°ë‚˜, ëª¨ë‘ ì†Œì§„ ì‹œ question_idsë¥¼ ì´ˆê¸°í™”í•´ ì¬ì¶œì œí•©ë‹ˆë‹¤.\n",
        "\n",
        "3. ì„ íƒëœ ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ëŠ” CustomRetriever í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ì—¬ LangChain ë¦¬íŠ¸ë¦¬ë²„ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ì‹œí‚µë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "IBFFamLF8ITb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_question_selector(documents: List[Document], session: SessionState):\n",
        "    def filter_by_level_and_seen(query: str):\n",
        "        level = session.current_level\n",
        "        filtered_docs = [\n",
        "            doc for doc in documents\n",
        "            if doc.metadata.get(\"level\") == level and hash(doc.page_content) not in session.question_ids\n",
        "        ]\n",
        "        if not filtered_docs:\n",
        "            for lvl in [\"ë‚œì´ë„ í•˜\", \"ë‚œì´ë„ ì¤‘\", \"ë‚œì´ë„ ìƒ\"]:\n",
        "                if lvl != level:\n",
        "                    filtered_docs = [\n",
        "                        doc for doc in documents\n",
        "                        if doc.metadata.get(\"level\") == lvl and hash(doc.page_content) not in session.question_ids\n",
        "                    ]\n",
        "                    if filtered_docs:\n",
        "                        session.current_level = lvl\n",
        "                        break\n",
        "            if not filtered_docs:\n",
        "                session.question_ids.clear()\n",
        "                filtered_docs = [doc for doc in documents if doc.metadata.get(\"level\") == session.current_level]\n",
        "        if filtered_docs:\n",
        "            selected = np.random.choice(filtered_docs)\n",
        "            session.question_ids.add(hash(selected.page_content))\n",
        "            return [selected]\n",
        "        return []\n",
        "    class CustomRetriever:\n",
        "        def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            return filter_by_level_and_seen(query)\n",
        "    return CustomRetriever()\n"
      ],
      "metadata": {
        "id": "Ea_vIIEZ8ItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜**\n",
        "1. ì¶œì œìš©ê³¼ í”¼ë“œë°±ìš© ë‘ ê°€ì§€ ChatPrompt í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. question_templateì€ ë¬¸ì œë§Œ í•™ìƒì—ê²Œ ë³´ì—¬ì£¼ê³ , feedback_templateì€ ì •ë‹µ íŒë‹¨ ë° í•´ì„¤ì— ë”°ë¼ LLMì—ê²Œ í”¼ë“œë°± ìƒì„± ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "IGIMUgGS8QEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "ë‹¤ìŒì€ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œì…ë‹ˆë‹¤:\n",
        "\n",
        "{question}\n",
        "\n",
        "ì´ ë¬¸ì œë¥¼ í•™ìƒì—ê²Œ ì¶œì œí•˜ë˜, ì •ë‹µê³¼ í•´ì„¤ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "\"\"\")\n",
        "\n",
        "feedback_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"ë‹¹ì‹ ì€ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "     í•™ìƒì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ìœ ìµí•œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    ë¬¸ì œ: {question}\n",
        "    í•™ìƒ ë‹µë³€: {user_answer}\n",
        "    ì •ë‹µ: {correct_answer}\n",
        "    ì •ë‹µ ì—¬ë¶€: {is_correct}\n",
        "    í•´ì„¤: {explanation}\n",
        "\n",
        "    ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n",
        "    1. í•™ìƒì˜ ì´í•´ë„ë¥¼ ê²©ë ¤í•˜ì„¸ìš”\n",
        "    2. ì™œ í•´ë‹¹ ë‹µë³€ì´ ë§ê±°ë‚˜ í‹€ë ¸ëŠ”ì§€ ê°„ëµíˆ ì„¤ëª…í•˜ì„¸ìš”\n",
        "    3. í•µì‹¬ ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”\n",
        "    \"\"\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "4ZcIv3t18QIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **í”¼ë“œë°± ì²´ì¸ ìƒì„± í•¨ìˆ˜**\n",
        "1. ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ LLMê³¼ feedback_templateì„ ì—°ê²°í•œ ì²´ì¸ì„ ìƒì„±í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.\n",
        "2. ì´í›„ë¡œëŠ” ìƒì„±ëœ ì²´ì¸ì„ ì¬í™œìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "G29i24S18b9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feedback_chain():\n",
        "    if not hasattr(get_feedback_chain, \"chain\"):\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "        get_feedback_chain.chain = (\n",
        "            RunnablePassthrough()\n",
        "            | feedback_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    return get_feedback_chain.chain\n"
      ],
      "metadata": {
        "id": "vUFvTWO-8cCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë¬¸ì œ ì„ íƒ ë° ì§ˆë¬¸ ì¤€ë¹„ í•¨ìˆ˜**\n",
        "1. select_next_question(): question_selectorë¥¼ í†µí•´ ë‹¤ìŒ ë¬¸ì œ Documentë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "\n",
        "2. prepare_next_question(): í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ë¬¸ìì—´ë¡œ ì¡°í•©í•˜ê³ , ë‹¤ìŒ ë¬¸ì œë§Œ í•™ìƒì—ê²Œ ë³´ì—¬ì¤„ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "v-iRatdE8g1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_next_question(session: SessionState, question_selector) -> Optional[Document]:\n",
        "    docs = question_selector.get_relevant_documents(\"next_question\")\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "def prepare_next_question(session: SessionState, question_selector) -> str:\n",
        "    next_doc = select_next_question(session, question_selector)\n",
        "    if not next_doc:\n",
        "        return \"**ëª¨ë“  ë¬¸ì œë¥¼ í’€ì—ˆìŠµë‹ˆë‹¤! ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.**\"\n",
        "    q, a, exp = process_question(next_doc.page_content)\n",
        "    session.current_question = next_doc.page_content\n",
        "    session.current_answer = a\n",
        "    q_only = hide_answer_from_question(q)\n",
        "    info = (f\"**í•™ìŠµ ì§„í–‰ ìƒí™©**\\n\"\n",
        "            f\"- í˜„ì¬ ë‚œì´ë„: {session.current_level}\\n\"\n",
        "            f\"- í‘¼ ë¬¸ì œ: {session.total_attempts}ë¬¸ì œ\\n\"\n",
        "            f\"- ë§ì¶˜ ë¬¸ì œ: {session.correct_count}ë¬¸ì œ\\n\"\n",
        "            f\"- í‹€ë¦° ë¬¸ì œ: {session.incorrect_count}ë¬¸ì œ\\n\\n\")\n",
        "    return info + f\"**ë‹¤ìŒ ë¬¸ì œ**\\n{q_only}\"\n"
      ],
      "metadata": {
        "id": "sa49Ktp98g8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **í”¼ë“œë°± ìƒì„± í•¨ìˆ˜**\n",
        "1. í•™ìƒ ë‹µì•ˆì„ ë°›ì•„ ì •ë‹µ ì—¬ë¶€ í”Œë˜ê·¸ì™€ í•¨ê»˜ LLM í”¼ë“œë°± ì²´ì¸ì— ì „ë‹¬í•˜ì—¬ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸ í”¼ë“œë°±ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ , ì˜¤ë‹µì¼ ê²½ìš° ì •ë‹µê³¼ í•´ì„¤ì„ ì¶”ê°€ë¡œ ë¶™ì—¬ì¤ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "L34YKScB8nQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feedback(session: SessionState, user_answer: str, is_correct: bool) -> str:\n",
        "    try:\n",
        "        question, _, exp = process_question(session.current_question)\n",
        "        feedback_input = {\n",
        "            \"question\": question,\n",
        "            \"user_answer\": user_answer,\n",
        "            \"correct_answer\": session.current_answer,\n",
        "            \"is_correct\": \"ì •ë‹µì…ë‹ˆë‹¤!\" if is_correct else \"ì˜¤ë‹µì…ë‹ˆë‹¤.\",\n",
        "            \"explanation\": exp if exp else \"ì¶”ê°€ í•´ì„¤ ì—†ìŒ\"\n",
        "        }\n",
        "        fb = get_feedback_chain().invoke(feedback_input)\n",
        "    except Exception as e:\n",
        "        print(f\"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        fb = (\"ì •ë‹µì…ë‹ˆë‹¤!\" if is_correct else \"ì˜¤ë‹µì…ë‹ˆë‹¤.\") + f\" ì •ë‹µì€ {session.current_answer}ì…ë‹ˆë‹¤.\"\n",
        "    if not is_correct:\n",
        "        fb += f\"\\n\\n**ì •ë‹µ**: {session.current_answer}\"\n",
        "    _, _, exp = process_question(session.current_question)\n",
        "    if exp:\n",
        "        fb += f\"\\n\\n**í•´ì„¤**: {exp}\"\n",
        "    return fb\n"
      ],
      "metadata": {
        "id": "boXYKqJE8nXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradio ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë¡œì§**\n",
        "ì‚¬ìš©ìê°€ ì²« ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•ŒëŠ” ì¸ì‚¬ì™€ ì²« ë¬¸ì œë¥¼ ë³´ì—¬ì£¼ê³ , ì´í›„ë¡œëŠ” ì‚¬ìš©ìì˜ ë‹µì•ˆì„ í‰ê°€Â·í”¼ë“œë°± ìƒì„±Â·ë‹¤ìŒ ë¬¸ì œ ì œì‹œ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0Izx-0hl8s3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(message, history, session_state, question_selector):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "    if not history:\n",
        "        doc = select_next_question(session_state, question_selector)\n",
        "        if not doc:\n",
        "            return [{\"role\": \"assistant\", \"content\": \"ë¬¸ì œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}], \"\"\n",
        "        q, a, _ = process_question(doc.page_content)\n",
        "        session_state.current_question = doc.page_content\n",
        "        session_state.current_answer = a\n",
        "        return [\n",
        "            {\"role\": \"assistant\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”! ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í’€ì´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\"},\n",
        "            {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "        ], \"\"\n",
        "    new_hist = history.copy()\n",
        "    user_ans = message.strip().upper()\n",
        "    new_hist.append({\"role\": \"user\", \"content\": user_ans})\n",
        "    correct = (user_ans == session_state.current_answer.upper())\n",
        "    session_state.update_stats(correct)\n",
        "    fb = generate_feedback(session_state, user_ans, correct)\n",
        "    nxt_q = prepare_next_question(session_state, question_selector)\n",
        "    new_hist.append({\"role\": \"assistant\", \"content\": fb + \"\\n\\n\" + nxt_q})\n",
        "    return new_hist, \"\"\n"
      ],
      "metadata": {
        "id": "xsrjjRPL8s9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë²„íŠ¼ í´ë¦­ ë° ì±„íŒ… ì´ˆê¸°í™” í•¨ìˆ˜**\n",
        "1. answer_click()ëŠ” ë²„íŠ¼ í´ë¦­ ì‹œ ë‹µì•ˆì„ ì²˜ë¦¬í•˜ê³  ì±„íŒ… ë‚´ì—­ì„ ê°±ì‹ í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. clear_chat()ëŠ” ì„¸ì…˜ê³¼ ì±„íŒ… ë‚´ì—­ì„ ì´ˆê¸°í™”í•˜ì—¬ ì²« ë¬¸ì œë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "USHMe0-a8w-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_click(answer, history, session_state, question_selector):\n",
        "    new_history, _ = chat_interface(answer, history, session_state, question_selector)\n",
        "    return new_history\n",
        "\n",
        "def clear_chat(session_state, question_selector):\n",
        "    new_sess = SessionState()\n",
        "    first_doc = select_next_question(new_sess, question_selector)\n",
        "    if not first_doc:\n",
        "        return new_sess, [{\"role\": \"assistant\", \"content\": \"ë¬¸ì œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}]\n",
        "    q, a, _ = process_question(first_doc.page_content)\n",
        "    new_sess.current_question = first_doc.page_content\n",
        "    new_sess.current_answer = a\n",
        "    return new_sess, [\n",
        "        {\"role\": \"assistant\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”! ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í’€ì´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\"},\n",
        "        {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "fNzb2tgS8xDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë° Gradio UI êµ¬ì„±**\n",
        "1. RAG ì²´ì¸Â·ì„¸ì…˜Â·ì§ˆë¬¸ ì„ íƒê¸°ë¥¼ ì¤€ë¹„í•œ í›„, Gradio Blocksë¡œ ì „ì²´ UIë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. ë‹µì•ˆ ë²„íŠ¼(A/B/C/D)ê³¼ ì´ˆê¸°í™” ë²„íŠ¼ì„ ë°°ì¹˜í•˜ê³ , ê° í´ë¦­ ì´ë²¤íŠ¸ì— ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.\n",
        "\n",
        "3. ì•±ì„ ì™¸ë¶€ ê³µìœ  ëª¨ë“œ(share=True)ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "yblJz5vL81V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    llm, retriever, documents = setup_rag_chain()\n",
        "    session_state = SessionState()\n",
        "    question_selector = create_question_selector(documents, session_state)\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "        .gradio-container {max-width: 900px; margin: auto;}\n",
        "        .chat-history {height: 600px !important;}\n",
        "        footer {display: none !important;}\n",
        "    \"\"\") as demo:\n",
        "        gr.Markdown(\"# ğŸ“ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í•™ìŠµ ì±—ë´‡\")\n",
        "        gr.Markdown(\"ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹µì•ˆì„ ì„ íƒí•˜ì„¸ìš”.\")\n",
        "        session_state_gr = gr.State(session_state)\n",
        "        question_selector_gr = gr.State(question_selector)\n",
        "        chatbot = gr.Chatbot(label=\"í•™ìŠµ ëŒ€í™”\", height=500, type=\"messages\", render_markdown=True)\n",
        "        with gr.Row():\n",
        "            btn_A = gr.Button(\"A\")\n",
        "            btn_B = gr.Button(\"B\")\n",
        "            btn_C = gr.Button(\"C\")\n",
        "            btn_D = gr.Button(\"D\")\n",
        "            clear_btn = gr.Button(\"ì´ˆê¸°í™”\")\n",
        "        btn_A.click(\n",
        "            lambda history, ss, qs: answer_click(\"A\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_B.click(\n",
        "            lambda history, ss, qs: answer_click(\"B\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_C.click(\n",
        "            lambda history, ss, qs: answer_click(\"C\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_D.click(\n",
        "            lambda history, ss, qs: answer_click(\"D\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        clear_btn.click(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "        demo.load(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "    demo.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "PSd83oG681dX",
        "outputId": "5843d51b-f8f0-46b4-816b-04ee30703f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS ì¸ë±ìŠ¤ì™€ ì²­í¬ ë§¤í•‘ì„ ë¡œë“œí•˜ëŠ” ì¤‘...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c55bdcff5ccafc76c9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c55bdcff5ccafc76c9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì „ì²´ì½”ë“œ"
      ],
      "metadata": {
        "id": "_0CuHAx2_KnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ì•„ë˜ì˜ ì½”ë“œë¥¼ êµ¬ê°„ ë³„ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•´ì¤˜. ë„ˆê°€ í•´ì¤€ ì„¤ëª…ê³¼ êµ¬ê°„í™”ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì˜COLAPì‘ì—…í™˜ê²½ì— ì…€ë¡œ êµ¬ë¶„í•˜ì—¬ ì˜¬ë¦´ê±°ì•¼.\n",
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import gradio as gr\n",
        "from google.colab import drive, userdata\n",
        "from typing import List, Dict, Optional, Any\n",
        "from langchain_community.vectorstores.faiss import FAISS as LangchainFAISS\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import Document\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import FakeEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "# ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° API í‚¤ ì„¤ì •\n",
        "drive.mount('/content/drive')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('Gemini_API_KEY')\n",
        "\n",
        "# ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì„¤ì •\n",
        "store_dir = '/content/drive/MyDrive/tutor_store'\n",
        "faiss_path = os.path.join(store_dir, \"medical_qa_index.faiss\")\n",
        "chunk_path = os.path.join(store_dir, \"chunk_mapping.pkl\")\n",
        "\n",
        "# ì„¸ì…˜ ìƒíƒœ ì €ì¥ í´ë˜ìŠ¤\n",
        "class SessionState:\n",
        "    def __init__(self):\n",
        "        self.current_level = \"ë‚œì´ë„ í•˜\"\n",
        "        self.correct_count = 0\n",
        "        self.incorrect_count = 0\n",
        "        self.current_question = None\n",
        "        self.current_answer = None\n",
        "        self.question_ids = set()\n",
        "\n",
        "    @property\n",
        "    def total_attempts(self):\n",
        "        return self.correct_count + self.incorrect_count\n",
        "\n",
        "    @property\n",
        "    def success_rate(self):\n",
        "        return (self.correct_count / self.total_attempts) if self.total_attempts else 0\n",
        "\n",
        "    def update_stats(self, is_correct: bool):\n",
        "        if is_correct:\n",
        "            self.correct_count += 1\n",
        "            if self.current_level == \"ë‚œì´ë„ í•˜\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"ë‚œì´ë„ ì¤‘\"\n",
        "            elif self.current_level == \"ë‚œì´ë„ ì¤‘\" and self.success_rate >= 0.7 and self.total_attempts >= 3:\n",
        "                self.current_level = \"ë‚œì´ë„ ìƒ\"\n",
        "        else:\n",
        "            self.incorrect_count += 1\n",
        "            if self.current_level == \"ë‚œì´ë„ ìƒ\" and self.success_rate < 0.5:\n",
        "                self.current_level = \"ë‚œì´ë„ ì¤‘\"\n",
        "            elif self.current_level == \"ë‚œì´ë„ ì¤‘\" and self.success_rate < 0.3:\n",
        "                self.current_level = \"ë‚œì´ë„ í•˜\"\n",
        "\n",
        "# ì˜í•™ ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
        "class MedicalQuestionSplitter(TextSplitter):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pattern = r'(ë‚œì´ë„ í•˜ \\d+\\.|ë‚œì´ë„ ì¤‘ \\d+\\.|ë‚œì´ë„ ìƒ \\d+\\.)'\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        splits = re.split(self.pattern, text)\n",
        "        chunks = []\n",
        "        for i in range(1, len(splits), 2):\n",
        "            if i+1 < len(splits):\n",
        "                chunk = splits[i] + splits[i+1]\n",
        "                chunks.append(chunk.strip())\n",
        "        return chunks\n",
        "\n",
        "# ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ì»¤ìŠ¤í…€ FakeEmbeddings\n",
        "class CustomFakeEmbeddings(FakeEmbeddings):\n",
        "    def __init__(self, size: int = 1024):\n",
        "        super().__init__(size=size)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return [np.zeros(self.size) for _ in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return np.zeros(self.size)\n",
        "\n",
        "# ë²¡í„°ìŠ¤í† ì–´ ë° ë¬¸ì œ ë¬¸ì„œ ë¡œë“œ\n",
        "def load_vectorstore():\n",
        "    print(\"FAISS ì¸ë±ìŠ¤ì™€ ì²­í¬ ë§¤í•‘ì„ ë¡œë“œí•˜ëŠ” ì¤‘...\")\n",
        "    index = faiss.read_index(faiss_path)\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        question_chunks = pickle.load(f)\n",
        "    documents = [Document(page_content=chunk, metadata={\"level\": get_level_from_text(chunk)}) for chunk in question_chunks]\n",
        "    embeddings = CustomFakeEmbeddings(size=index.d)\n",
        "    vectorstore = LangchainFAISS(\n",
        "        embedding_function=embeddings,\n",
        "        index=index,\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={}\n",
        "    )\n",
        "    vectorstore.docstore._dict = {str(i): doc for i, doc in enumerate(documents)}\n",
        "    return vectorstore, documents\n",
        "\n",
        "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
        "def get_level_from_text(text: str) -> str:\n",
        "    if text.startswith(\"ë‚œì´ë„ í•˜\"): return \"ë‚œì´ë„ í•˜\"\n",
        "    if text.startswith(\"ë‚œì´ë„ ì¤‘\"): return \"ë‚œì´ë„ ì¤‘\"\n",
        "    if text.startswith(\"ë‚œì´ë„ ìƒ\"): return \"ë‚œì´ë„ ìƒ\"\n",
        "    match = re.match(r'(ë‚œì´ë„\\s*[í•˜ì¤‘ìƒ])', text)\n",
        "    return match.group(1) if match else \"ë‚œì´ë„ í•˜\"\n",
        "\n",
        "def process_question(text: str) -> tuple:\n",
        "    if \"ì •ë‹µ:\" not in text:\n",
        "        return text, \"\", \"\"\n",
        "    q_part = text.split(\"ì •ë‹µ:\")[0].strip()\n",
        "    a_full = text.split(\"ì •ë‹µ:\")[1].split(\"\\n\")[0].strip()\n",
        "    a_part = a_full.split('.')[0].strip() if '.' in a_full else a_full\n",
        "    exp = text.split(\"í•´ì„¤:\")[1].strip() if \"í•´ì„¤:\" in text else \"\"\n",
        "    return q_part, a_part, exp\n",
        "\n",
        "def hide_answer_from_question(text: str) -> str:\n",
        "    return text.split(\"ì •ë‹µ:\")[0].strip() if \"ì •ë‹µ:\" in text else text\n",
        "\n",
        "# LLM ë° RAG ì„¤ì •\n",
        "def setup_rag_chain():\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "    vectorstore, documents = load_vectorstore()\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    return llm, retriever, documents\n",
        "\n",
        "# ë¬¸ì œ ì„ íƒ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±\n",
        "def create_question_selector(documents: List[Document], session: SessionState):\n",
        "    def filter_by_level_and_seen(query: str):\n",
        "        level = session.current_level\n",
        "        filtered_docs = [\n",
        "            doc for doc in documents\n",
        "            if doc.metadata.get(\"level\") == level and hash(doc.page_content) not in session.question_ids\n",
        "        ]\n",
        "        if not filtered_docs:\n",
        "            for lvl in [\"ë‚œì´ë„ í•˜\", \"ë‚œì´ë„ ì¤‘\", \"ë‚œì´ë„ ìƒ\"]:\n",
        "                if lvl != level:\n",
        "                    filtered_docs = [\n",
        "                        doc for doc in documents\n",
        "                        if doc.metadata.get(\"level\") == lvl and hash(doc.page_content) not in session.question_ids\n",
        "                    ]\n",
        "                    if filtered_docs:\n",
        "                        session.current_level = lvl\n",
        "                        break\n",
        "            if not filtered_docs:\n",
        "                session.question_ids.clear()\n",
        "                filtered_docs = [doc for doc in documents if doc.metadata.get(\"level\") == session.current_level]\n",
        "        if filtered_docs:\n",
        "            selected = np.random.choice(filtered_docs)\n",
        "            session.question_ids.add(hash(selected.page_content))\n",
        "            return [selected]\n",
        "        return []\n",
        "    class CustomRetriever:\n",
        "        def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            return filter_by_level_and_seen(query)\n",
        "    return CustomRetriever()\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "question_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "ë‹¤ìŒì€ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œì…ë‹ˆë‹¤:\n",
        "\n",
        "{question}\n",
        "\n",
        "ì´ ë¬¸ì œë¥¼ í•™ìƒì—ê²Œ ì¶œì œí•˜ë˜, ì •ë‹µê³¼ í•´ì„¤ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "\"\"\")\n",
        "\n",
        "feedback_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"ë‹¹ì‹ ì€ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "     í•™ìƒì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ìœ ìµí•œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.\"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    ë¬¸ì œ: {question}\n",
        "    í•™ìƒ ë‹µë³€: {user_answer}\n",
        "    ì •ë‹µ: {correct_answer}\n",
        "    ì •ë‹µ ì—¬ë¶€: {is_correct}\n",
        "    í•´ì„¤: {explanation}\n",
        "\n",
        "    ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n",
        "    1. í•™ìƒì˜ ì´í•´ë„ë¥¼ ê²©ë ¤í•˜ì„¸ìš”\n",
        "    2. ì™œ í•´ë‹¹ ë‹µë³€ì´ ë§ê±°ë‚˜ í‹€ë ¸ëŠ”ì§€ ê°„ëµíˆ ì„¤ëª…í•˜ì„¸ìš”\n",
        "    3. í•µì‹¬ ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "# í”¼ë“œë°± ì²´ì¸ ìƒì„± í•¨ìˆ˜ (ì „ì—­ ê°ì²´ë¡œ ìºì‹±)\n",
        "def get_feedback_chain():\n",
        "    if not hasattr(get_feedback_chain, \"chain\"):\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "        get_feedback_chain.chain = (\n",
        "            RunnablePassthrough()\n",
        "            | feedback_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    return get_feedback_chain.chain\n",
        "\n",
        "# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜\n",
        "def select_next_question(session: SessionState, question_selector) -> Optional[Document]:\n",
        "    docs = question_selector.get_relevant_documents(\"next_question\")\n",
        "    return docs[0] if docs else None\n",
        "\n",
        "def prepare_next_question(session: SessionState, question_selector) -> str:\n",
        "    next_doc = select_next_question(session, question_selector)\n",
        "    if not next_doc:\n",
        "        return \"**ëª¨ë“  ë¬¸ì œë¥¼ í’€ì—ˆìŠµë‹ˆë‹¤! ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.**\"\n",
        "    q, a, exp = process_question(next_doc.page_content)\n",
        "    session.current_question = next_doc.page_content\n",
        "    session.current_answer = a\n",
        "    q_only = hide_answer_from_question(q)\n",
        "    info = (f\"**í•™ìŠµ ì§„í–‰ ìƒí™©**\\n\"\n",
        "            f\"- í˜„ì¬ ë‚œì´ë„: {session.current_level}\\n\"\n",
        "            f\"- í‘¼ ë¬¸ì œ: {session.total_attempts}ë¬¸ì œ\\n\"\n",
        "            f\"- ë§ì¶˜ ë¬¸ì œ: {session.correct_count}ë¬¸ì œ\\n\"\n",
        "            f\"- í‹€ë¦° ë¬¸ì œ: {session.incorrect_count}ë¬¸ì œ\\n\\n\")\n",
        "    return info + f\"**ë‹¤ìŒ ë¬¸ì œ**\\n{q_only}\"\n",
        "\n",
        "def generate_feedback(session: SessionState, user_answer: str, is_correct: bool) -> str:\n",
        "    try:\n",
        "        question, _, exp = process_question(session.current_question)\n",
        "        feedback_input = {\n",
        "            \"question\": question,\n",
        "            \"user_answer\": user_answer,\n",
        "            \"correct_answer\": session.current_answer,\n",
        "            \"is_correct\": \"ì •ë‹µì…ë‹ˆë‹¤!\" if is_correct else \"ì˜¤ë‹µì…ë‹ˆë‹¤.\",\n",
        "            \"explanation\": exp if exp else \"ì¶”ê°€ í•´ì„¤ ì—†ìŒ\"\n",
        "        }\n",
        "        fb = get_feedback_chain().invoke(feedback_input)\n",
        "    except Exception as e:\n",
        "        print(f\"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        fb = (\"ì •ë‹µì…ë‹ˆë‹¤!\" if is_correct else \"ì˜¤ë‹µì…ë‹ˆë‹¤.\") + f\" ì •ë‹µì€ {session.current_answer}ì…ë‹ˆë‹¤.\"\n",
        "    if not is_correct:\n",
        "        fb += f\"\\n\\n**ì •ë‹µ**: {session.current_answer}\"\n",
        "    _, _, exp = process_question(session.current_question)\n",
        "    if exp:\n",
        "        fb += f\"\\n\\n**í•´ì„¤**: {exp}\"\n",
        "    return fb\n",
        "\n",
        "# Gradio UI í•¨ìˆ˜\n",
        "def chat_interface(message, history, session_state, question_selector):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "    if not history:\n",
        "        doc = select_next_question(session_state, question_selector)\n",
        "        if not doc:\n",
        "            return [{\"role\": \"assistant\", \"content\": \"ë¬¸ì œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}], \"\"\n",
        "        q, a, _ = process_question(doc.page_content)\n",
        "        session_state.current_question = doc.page_content\n",
        "        session_state.current_answer = a\n",
        "        return [\n",
        "            {\"role\": \"assistant\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”! ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í’€ì´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\"},\n",
        "            {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "        ], \"\"\n",
        "    new_hist = history.copy()\n",
        "    user_ans = message.strip().upper()\n",
        "    new_hist.append({\"role\": \"user\", \"content\": user_ans})\n",
        "    correct = (user_ans == session_state.current_answer.upper())\n",
        "    session_state.update_stats(correct)\n",
        "    fb = generate_feedback(session_state, user_ans, correct)\n",
        "    nxt_q = prepare_next_question(session_state, question_selector)\n",
        "    new_hist.append({\"role\": \"assistant\", \"content\": fb + \"\\n\\n\" + nxt_q})\n",
        "    return new_hist, \"\"\n",
        "\n",
        "def answer_click(answer, history, session_state, question_selector):\n",
        "    new_history, _ = chat_interface(answer, history, session_state, question_selector)\n",
        "    return new_history\n",
        "\n",
        "def clear_chat(session_state, question_selector):\n",
        "    new_sess = SessionState()\n",
        "    first_doc = select_next_question(new_sess, question_selector)\n",
        "    if not first_doc:\n",
        "        return new_sess, [{\"role\": \"assistant\", \"content\": \"ë¬¸ì œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}]\n",
        "    q, a, _ = process_question(first_doc.page_content)\n",
        "    new_sess.current_question = first_doc.page_content\n",
        "    new_sess.current_answer = a\n",
        "    return new_sess, [\n",
        "        {\"role\": \"assistant\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”! ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í’€ì´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\"},\n",
        "        {\"role\": \"assistant\", \"content\": hide_answer_from_question(q)}\n",
        "    ]\n",
        "\n",
        "# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜\n",
        "def main():\n",
        "    llm, retriever, documents = setup_rag_chain()\n",
        "    session_state = SessionState()\n",
        "    question_selector = create_question_selector(documents, session_state)\n",
        "    with gr.Blocks(css=\"\"\"\n",
        "        .gradio-container {max-width: 900px; margin: auto;}\n",
        "        .chat-history {height: 600px !important;}\n",
        "        footer {display: none !important;}\n",
        "    \"\"\") as demo:\n",
        "        gr.Markdown(\"# ğŸ“ ì‘ì—…ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ë¬¸ì œ í•™ìŠµ ì±—ë´‡\")\n",
        "        gr.Markdown(\"ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹µì•ˆì„ ì„ íƒí•˜ì„¸ìš”.\")\n",
        "        session_state_gr = gr.State(session_state)\n",
        "        question_selector_gr = gr.State(question_selector)\n",
        "        chatbot = gr.Chatbot(label=\"í•™ìŠµ ëŒ€í™”\", height=500, type=\"messages\", render_markdown=True)\n",
        "        with gr.Row():\n",
        "            btn_A = gr.Button(\"A\")\n",
        "            btn_B = gr.Button(\"B\")\n",
        "            btn_C = gr.Button(\"C\")\n",
        "            btn_D = gr.Button(\"D\")\n",
        "            clear_btn = gr.Button(\"ì´ˆê¸°í™”\")\n",
        "        btn_A.click(\n",
        "            lambda history, ss, qs: answer_click(\"A\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_B.click(\n",
        "            lambda history, ss, qs: answer_click(\"B\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_C.click(\n",
        "            lambda history, ss, qs: answer_click(\"C\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        btn_D.click(\n",
        "            lambda history, ss, qs: answer_click(\"D\", history, ss, qs),\n",
        "            inputs=[chatbot, session_state_gr, question_selector_gr],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "        clear_btn.click(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "        demo.load(\n",
        "            lambda ss, qs: clear_chat(ss, qs),\n",
        "            inputs=[session_state_gr, question_selector_gr],\n",
        "            outputs=[session_state_gr, chatbot]\n",
        "        )\n",
        "    demo.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6KXSVCOS-7l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}